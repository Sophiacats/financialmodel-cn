import streamlit as st
import yfinance as yf
import requests
import feedparser
from datetime import datetime, timedelta
import re
from urllib.parse import quote
import warnings
warnings.filterwarnings('ignore')

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ“° å¤šæºæ–°é—»é›†æˆç³»ç»Ÿ",
    page_icon="ğŸ“°",
    layout="wide"
)

st.title("ğŸ“° å¤šæºæ–°é—»é›†æˆç³»ç»Ÿ")
st.markdown("**æ•´åˆå¤šä¸ªçœŸå®æ–°é—»æº + å»é‡ + å®æ—¶æ›´æ–°**")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'news_data' not in st.session_state:
    st.session_state.news_data = None
if 'source_stats' not in st.session_state:
    st.session_state.source_stats = {}

# ==================== æ–°é—»æº1: yfinance ====================
def fetch_yfinance_news(ticker, debug=False):
    """yfinanceæ–°é—»è·å–ï¼ˆå·²éªŒè¯æœ‰æ•ˆï¼‰"""
    try:
        stock = yf.Ticker(ticker)
        raw_news = stock.news
        
        if not raw_news:
            return []
        
        processed_news = []
        for i, article in enumerate(raw_news):
            try:
                if not isinstance(article, dict):
                    continue
                
                # å¤„ç†æ–°çš„APIç»“æ„
                content_data = article.get('content', article)
                
                # æå–æ ‡é¢˜
                title = ''
                title_sources = [
                    content_data.get('title', ''),
                    content_data.get('headline', ''),
                    article.get('title', ''),
                ]
                
                for t in title_sources:
                    if t and len(str(t).strip()) > 10:
                        title = str(t).strip()
                        break
                
                if not title:
                    continue
                
                # æå–æ‘˜è¦
                summary = ''
                summary_sources = [
                    content_data.get('summary', ''),
                    content_data.get('description', ''),
                    article.get('summary', ''),
                ]
                
                for s in summary_sources:
                    if s and len(str(s).strip()) > 10:
                        summary = str(s).strip()
                        break
                
                # æå–URL
                url = ''
                url_sources = [
                    content_data.get('clickThroughUrl', {}).get('url', '') if isinstance(content_data.get('clickThroughUrl'), dict) else content_data.get('clickThroughUrl', ''),
                    content_data.get('link', ''),
                    article.get('link', ''),
                ]
                
                for u in url_sources:
                    if u and isinstance(u, str) and len(u) > 10:
                        url = u
                        break
                
                # æå–æ—¶é—´
                published_time = datetime.now() - timedelta(hours=i+1)
                time_sources = [
                    content_data.get('providerPublishTime'),
                    article.get('providerPublishTime'),
                ]
                
                for time_val in time_sources:
                    if time_val:
                        try:
                            published_time = datetime.fromtimestamp(time_val)
                            break
                        except:
                            continue
                
                processed_news.append({
                    'title': title,
                    'summary': summary or 'æš‚æ— æ‘˜è¦',
                    'url': url,
                    'source': 'Yahoo Finance',
                    'published': published_time,
                    'method': 'yfinance'
                })
                
            except Exception as e:
                if debug:
                    st.error(f"yfinanceå¤„ç†ç¬¬{i+1}æ¡æ–°é—»å¤±è´¥: {str(e)}")
                continue
        
        return processed_news
        
    except Exception as e:
        if debug:
            st.error(f"yfinanceè·å–å¤±è´¥: {str(e)}")
        return []

# ==================== æ–°é—»æº2: RSSæº ====================
def fetch_rss_news(ticker=None, debug=False):
    """RSSæ–°é—»æºè·å–"""
    rss_sources = [
        {
            'name': 'Reuters Business',
            'url': 'http://feeds.reuters.com/reuters/businessNews',
            'encoding': 'utf-8'
        },
        {
            'name': 'MarketWatch',
            'url': 'https://feeds.marketwatch.com/marketwatch/topstories/',
            'encoding': 'utf-8'
        },
        {
            'name': 'CNN Business',
            'url': 'http://rss.cnn.com/rss/money_latest.rss',
            'encoding': 'utf-8'
        }
    ]
    
    all_rss_news = []
    
    for source in rss_sources:
        try:
            if debug:
                st.write(f"ğŸ” è·å– {source['name']} RSS...")
            
            feed = feedparser.parse(source['url'])
            
            if not feed.entries:
                if debug:
                    st.warning(f"âš ï¸ {source['name']}: æ— æ•°æ®")
                continue
            
            source_news = []
            for entry in feed.entries[:5]:  # æ¯ä¸ªæºå–5æ¡
                try:
                    title = entry.get('title', '')
                    summary = entry.get('summary', '') or entry.get('description', '')
                    link = entry.get('link', '')
                    
                    # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œè¿‡æ»¤ç›¸å…³æ–°é—»
                    if ticker:
                        title_summary = (title + ' ' + summary).lower()
                        if ticker.lower() not in title_summary:
                            continue
                    
                    # å¤„ç†å‘å¸ƒæ—¶é—´
                    published_time = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            published_time = datetime(*entry.published_parsed[:6])
                        except:
                            pass
                    
                    if title and len(title) > 10:
                        source_news.append({
                            'title': title,
                            'summary': summary[:300] + '...' if len(summary) > 300 else summary,
                            'url': link,
                            'source': source['name'],
                            'published': published_time,
                            'method': 'RSS'
                        })
                        
                except Exception as e:
                    continue
            
            all_rss_news.extend(source_news)
            
            if debug:
                st.success(f"âœ… {source['name']}: è·å– {len(source_news)} æ¡æ–°é—»")
                
        except Exception as e:
            if debug:
                st.error(f"âŒ {source['name']}: {str(e)}")
            continue
    
    return all_rss_news

# ==================== æ–°é—»æº3: Google News ====================
def fetch_google_news(query, debug=False):
    """Google News RSSè·å–"""
    try:
        if debug:
            st.write(f"ğŸ” è·å–Google News: {query}")
        
        encoded_query = quote(query)
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        feed = feedparser.parse(url)
        
        if not feed.entries:
            if debug:
                st.warning("âš ï¸ Google News: æ— æ•°æ®")
            return []
        
        google_news = []
        for entry in feed.entries[:8]:  # å–8æ¡
            try:
                title = entry.get('title', '')
                summary = entry.get('summary', '') or 'æ¥è‡ªGoogle News'
                link = entry.get('link', '')
                
                # å¤„ç†å‘å¸ƒæ—¶é—´
                published_time = datetime.now()
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        published_time = datetime(*entry.published_parsed[:6])
                    except:
                        pass
                
                if title and len(title) > 10:
                    google_news.append({
                        'title': title,
                        'summary': summary[:300] + '...' if len(summary) > 300 else summary,
                        'url': link,
                        'source': 'Google News',
                        'published': published_time,
                        'method': 'Google RSS'
                    })
                    
            except Exception as e:
                continue
        
        if debug:
            st.success(f"âœ… Google News: è·å– {len(google_news)} æ¡æ–°é—»")
        
        return google_news
        
    except Exception as e:
        if debug:
            st.error(f"âŒ Google News: {str(e)}")
        return []

# ==================== æ–°é—»æ•´åˆå’Œå»é‡ ====================
def remove_duplicate_news(news_list):
    """å»é™¤é‡å¤æ–°é—»"""
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        # ä½¿ç”¨æ ‡é¢˜çš„å‰60ä¸ªå­—ç¬¦ä½œä¸ºå»é‡æ ‡è¯†
        title_key = news['title'][:60].lower().strip()
        
        if title_key not in seen_titles:
            seen_titles.add(title_key)
            unique_news.append(news)
    
    return unique_news

@st.cache_data(ttl=900)  # 15åˆ†é’Ÿç¼“å­˜
def get_comprehensive_news(ticker=None, debug=False):
    """è·å–æ‰€æœ‰æ¥æºçš„æ–°é—»å¹¶æ•´åˆ"""
    all_news = []
    source_stats = {}
    
    # æ¥æº1: yfinanceï¼ˆå¦‚æœæœ‰tickerï¼‰
    if ticker:
        yf_news = fetch_yfinance_news(ticker, debug)
        all_news.extend(yf_news)
        source_stats['yfinance'] = len(yf_news)
    
    # æ¥æº2: RSSæ–°é—»æº
    rss_news = fetch_rss_news(ticker, debug)
    all_news.extend(rss_news)
    source_stats['RSS'] = len(rss_news)
    
    # æ¥æº3: Google News
    if ticker:
        google_query = f"{ticker} stock news"
    else:
        google_query = "stock market financial news"
    
    google_news = fetch_google_news(google_query, debug)
    all_news.extend(google_news)
    source_stats['Google News'] = len(google_news)
    
    # å»é‡
    unique_news = remove_duplicate_news(all_news)
    
    # æŒ‰æ—¶é—´æ’åº
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_before = len(all_news)
    total_after = len(unique_news)
    removed = total_before - total_after
    
    if debug:
        st.info(f"ğŸ“Š æ€»è·å–: {total_before} æ¡ï¼Œå»é‡å: {total_after} æ¡ï¼Œç§»é™¤é‡å¤: {removed} æ¡")
    
    return unique_news, source_stats

# ==================== ç¿»è¯‘å’Œåˆ†æ ====================
def translate_finance_terms(text):
    """åŸºç¡€è´¢ç»æœ¯è¯­ç¿»è¯‘"""
    if not text:
        return text
    
    terms = {
        'earnings': 'è´¢æŠ¥', 'revenue': 'è¥æ”¶', 'profit': 'åˆ©æ¶¦',
        'stock': 'è‚¡ç¥¨', 'shares': 'è‚¡ä»·', 'market': 'å¸‚åœº',
        'announced': 'å®£å¸ƒ', 'reported': 'æŠ¥å‘Š', 'increased': 'å¢é•¿',
        'decreased': 'ä¸‹é™', 'beat': 'è¶…è¿‡', 'missed': 'æœªè¾¾åˆ°'
    }
    
    result = text
    for en, zh in terms.items():
        result = re.sub(r'\b' + re.escape(en) + r'\b', zh, result, flags=re.IGNORECASE)
    
    return result

def analyze_sentiment(title, summary):
    """ç®€å•æƒ…ç»ªåˆ†æ"""
    text = (title + ' ' + summary).lower()
    
    positive = ['beat', 'strong', 'growth', 'increase', 'rise', 'gain', 'success']
    negative = ['miss', 'weak', 'decline', 'fall', 'drop', 'loss', 'concern']
    
    pos_count = sum(1 for word in positive if word in text)
    neg_count = sum(1 for word in negative if word in text)
    
    if pos_count > neg_count:
        return 'åˆ©å¥½'
    elif neg_count > pos_count:
        return 'åˆ©ç©º'
    else:
        return 'ä¸­æ€§'

# ==================== ä¾§è¾¹æ  ====================
with st.sidebar:
    st.header("ğŸ“° å¤šæºæ–°é—»è®¾ç½®")
    
    ticker = st.text_input(
        "è‚¡ç¥¨ä»£ç  (å¯é€‰):",
        placeholder="ä¾‹å¦‚: AAPL, TSLA...",
        help="è¾“å…¥å…·ä½“ä»£ç è·å–ç›¸å…³æ–°é—»ï¼Œç•™ç©ºè·å–å¸‚åœºç»¼åˆæ–°é—»"
    ).upper().strip()
    
    st.markdown("---")
    
    debug_mode = st.checkbox("ğŸ”§ è°ƒè¯•æ¨¡å¼", help="æ˜¾ç¤ºè¯¦ç»†è·å–è¿‡ç¨‹")
    show_source = st.checkbox("ğŸ“¡ æ˜¾ç¤ºæ•°æ®æº", value=True, help="æ˜¾ç¤ºæ¯æ¡æ–°é—»çš„æ¥æº")
    
    st.markdown("---")
    
    if st.button("ğŸ“° è·å–å¤šæºæ–°é—»", type="primary"):
        with st.spinner("æ­£åœ¨ä»å¤šä¸ªçœŸå®æ–°é—»æºè·å–æ•°æ®..."):
            news_data, stats = get_comprehensive_news(ticker, debug_mode)
            st.session_state.news_data = news_data
            st.session_state.source_stats = stats
    
    if st.button("ğŸ”„ æ¸…é™¤ç¼“å­˜"):
        get_comprehensive_news.clear()
        st.session_state.news_data = None
        st.session_state.source_stats = {}
        st.success("ç¼“å­˜å·²æ¸…é™¤")

# ==================== ä¸»ç•Œé¢ ====================
if st.session_state.news_data is not None:
    news_data = st.session_state.news_data
    source_stats = st.session_state.source_stats
    
    if len(news_data) > 0:
        # æ•°æ®æºç»Ÿè®¡
        st.subheader("ğŸ“Š æ•°æ®æºç»Ÿè®¡")
        
        cols = st.columns(len(source_stats) + 1)
        
        total_raw = sum(source_stats.values())
        with cols[0]:
            st.metric("æ€»è®¡", f"{len(news_data)} æ¡", f"åŸå§‹: {total_raw}")
        
        for i, (source, count) in enumerate(source_stats.items(), 1):
            with cols[i]:
                st.metric(source, f"{count} æ¡")
        
        # æƒ…ç»ªåˆ†æç»Ÿè®¡
        sentiments = {}
        for news in news_data:
            sentiment = analyze_sentiment(news['title'], news['summary'])
            sentiments[sentiment] = sentiments.get(sentiment, 0) + 1
        
        if sentiments:
            st.subheader("ğŸ“ˆ å¸‚åœºæƒ…ç»ªåˆ†æ")
            sentiment_cols = st.columns(3)
            
            colors = {'åˆ©å¥½': 'green', 'åˆ©ç©º': 'red', 'ä¸­æ€§': 'gray'}
            for i, (sentiment, count) in enumerate(sentiments.items()):
                with sentiment_cols[i % 3]:
                    pct = count / len(news_data) * 100
                    st.metric(sentiment, count, f"{pct:.0f}%")
        
        st.markdown("---")
        
        # æ–°é—»åˆ—è¡¨
        st.subheader(f"ğŸ“° {ticker or 'å¸‚åœº'} æœ€æ–°æ–°é—» ({len(news_data)} æ¡)")
        
        for i, news in enumerate(news_data):
            with st.container():
                # æ ‡é¢˜
                title_display = news['title']
                if show_source:
                    title_display += f" [{news['source']}]"
                
                st.markdown(f"### {i+1}. {title_display}")
                
                # å…ƒä¿¡æ¯
                time_str = news['published'].strftime('%Y-%m-%d %H:%M')
                method_info = f" | æ–¹æ³•: {news['method']}" if show_source else ""
                st.caption(f"ğŸ•’ {time_str} | ğŸ“¡ {news['source']}{method_info}")
                
                # å†…å®¹
                col_main, col_side = st.columns([3, 1])
                
                with col_main:
                    # æ˜¾ç¤ºæ‘˜è¦ï¼ˆå¸¦åŸºç¡€ç¿»è¯‘ï¼‰
                    translated_summary = translate_finance_terms(news['summary'])
                    st.write(translated_summary)
                    
                    if news['url']:
                        st.markdown(f"ğŸ”— [é˜…è¯»åŸæ–‡]({news['url']})")
                
                with col_side:
                    # æƒ…ç»ªåˆ†æ
                    sentiment = analyze_sentiment(news['title'], news['summary'])
                    color_map = {'åˆ©å¥½': 'green', 'åˆ©ç©º': 'red', 'ä¸­æ€§': 'gray'}
                    st.markdown(f"**æƒ…ç»ª:** <span style='color:{color_map[sentiment]}'>{sentiment}</span>", unsafe_allow_html=True)
                
                st.markdown("---")
    
    else:
        st.warning("ğŸ“­ æœªè·å–åˆ°æ–°é—»æ•°æ®")
        if st.session_state.source_stats:
            st.write("æ•°æ®æºå°è¯•ç»“æœ:")
            for source, count in st.session_state.source_stats.items():
                st.write(f"- {source}: {count} æ¡")

else:
    st.markdown("""
    ## ğŸ¯ å¤šæºæ–°é—»é›†æˆç³»ç»Ÿ
    
    ### ğŸ“¡ **æ•´åˆå¤šä¸ªçœŸå®æ–°é—»æº**
    
    #### ğŸ¥‡ **ä¸»è¦æº - yfinance**
    - âœ… **é«˜è´¨é‡**: ç›´æ¥ç›¸å…³çš„è´¢ç»æ–°é—»
    - âœ… **ç»“æ„åŒ–**: æ•°æ®å®Œæ•´ï¼ŒåŒ…å«é“¾æ¥
    - âœ… **å®æ—¶æ€§**: æ›´æ–°åŠæ—¶
    - âš ï¸ **å±€é™æ€§**: ä»…Yahoo Financeè§†è§’
    
    #### ğŸ¥ˆ **è¡¥å……æº - RSS**
    - ğŸ“° **Reuters**: æƒå¨å›½é™…è´¢ç»æ–°é—»
    - ğŸ“Š **MarketWatch**: ä¸“ä¸šå¸‚åœºåˆ†æ
    - ğŸ¢ **CNN Business**: ä¸»æµå•†ä¸šæ–°é—»
    - âœ… **å¤šå…ƒåŒ–**: ä¸åŒåª’ä½“è§†è§’
    
    #### ğŸ¥‰ **æ‰©å±•æº - Google News**
    - ğŸ” **å…¨é¢æœç´¢**: èšåˆå¤šä¸ªæ–°é—»æº
    - ğŸŒ **å¹¿è¦†ç›–**: åŒ…å«å°ä¼—ä½†é‡è¦çš„æ–°é—»
    - âš¡ **å®æ—¶æ€§**: Googleæ–°é—»æ›´æ–°å¾ˆå¿«
    
    ### ğŸ›¡ï¸ **ç³»ç»Ÿä¼˜åŠ¿**
    
    #### ğŸ“Š **å¯é æ€§**
    - **å¤šé‡å¤‡ä»½**: ä¸€ä¸ªæºå¤±æ•ˆä¸å½±å“æ•´ä½“
    - **è‡ªåŠ¨å»é‡**: é¿å…é‡å¤æ–°é—»
    - **æ™ºèƒ½æ’åº**: æŒ‰æ—¶é—´å’Œé‡è¦æ€§æ’åˆ—
    
    #### ğŸ¯ **å…¨é¢æ€§**
    - **ä¸åŒè§†è§’**: å¤šä¸ªåª’ä½“çš„æŠ¥é“è§’åº¦
    - **æ›´å¤šæ•°é‡**: é€šå¸¸èƒ½è·å–20-30æ¡æ–°é—»
    - **äº’è¡¥ä¿¡æ¯**: æŸäº›æ–°é—»åªåœ¨ç‰¹å®šæºå‡ºç°
    
    ### ğŸ’¡ **ä½¿ç”¨å»ºè®®**
    
    - **ä¸ªè‚¡æ–°é—»**: è¾“å…¥è‚¡ç¥¨ä»£ç è·å–ç›¸å…³æ–°é—»
    - **å¸‚åœºæ–°é—»**: ç•™ç©ºè·å–ç»¼åˆå¸‚åœºæ–°é—»  
    - **å¯¹æ¯”éªŒè¯**: åŒä¸€äº‹ä»¶çš„å¤šæºæŠ¥é“å¢åŠ å¯ä¿¡åº¦
    - **æ—¶æ•ˆæŠŠæ¡**: å…³æ³¨å‘å¸ƒæ—¶é—´ï¼Œæœ€æ–°æ¶ˆæ¯æœ€é‡è¦
    
    ---
    
    **ğŸ‘ˆ åœ¨å·¦ä¾§å¼€å§‹ä½“éªŒå¤šæºæ–°é—»è·å–**
    """)

# é¡µè„š
st.markdown("---")
st.markdown("ğŸ“° å¤šæºæ–°é—»é›†æˆç³»ç»Ÿ | ğŸ”„ yfinance + RSS + Google News | ğŸš« å»é‡å¤„ç† | âš¡ å®æ—¶æ›´æ–°")
