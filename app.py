import streamlit as st
import requests
import re
from datetime import datetime, timedelta
import json
from urllib.parse import quote

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ“° ç®€å•æ–°é—»è·å–å™¨",
    page_icon="ğŸ“°",
    layout="wide"
)

st.title("ğŸ“° ç®€å•æ–°é—»è·å–å™¨")
st.markdown("**ä¸“æ³¨è·å–çœŸå®æ–°é—» - è°ƒè¯•ä¼˜å…ˆ**")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'news_results' not in st.session_state:
    st.session_state.news_results = None
if 'debug_logs' not in st.session_state:
    st.session_state.debug_logs = []

def log_debug(message):
    """è®°å½•è°ƒè¯•ä¿¡æ¯"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    st.session_state.debug_logs.append(f"[{timestamp}] {message}")

def test_basic_rss(url, source_name):
    """æµ‹è¯•åŸºæœ¬RSSè·å–"""
    log_debug(f"å¼€å§‹æµ‹è¯• {source_name}: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, timeout=15, headers=headers)
        log_debug(f"{source_name} - çŠ¶æ€ç : {response.status_code}")
        log_debug(f"{source_name} - å†…å®¹é•¿åº¦: {len(response.text)}")
        
        if response.status_code != 200:
            log_debug(f"{source_name} - HTTPé”™è¯¯: {response.status_code}")
            return []
        
        content = response.text
        
        # ç®€å•æ£€æŸ¥æ˜¯å¦æ˜¯RSSå†…å®¹
        if not ('<rss' in content.lower() or '<feed' in content.lower() or '<channel' in content.lower()):
            log_debug(f"{source_name} - ä¸æ˜¯æœ‰æ•ˆçš„RSSå†…å®¹")
            return []
        
        log_debug(f"{source_name} - æ£€æµ‹åˆ°æœ‰æ•ˆRSSå†…å®¹")
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ–°é—»é¡¹ç›®
        news_items = extract_news_from_rss(content, source_name)
        log_debug(f"{source_name} - æå–åˆ° {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        log_debug(f"{source_name} - å¼‚å¸¸: {str(e)}")
        return []

def extract_news_from_rss(content, source_name):
    """ä»RSSå†…å®¹ä¸­æå–æ–°é—»"""
    news_items = []
    
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ <item> æ ‡ç­¾
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        log_debug(f"{source_name} - æ‰¾åˆ° {len(items)} ä¸ªitemæ ‡ç­¾")
        
        for i, item in enumerate(items[:10]):  # åªå–å‰10ä¸ª
            try:
                # æå–æ ‡é¢˜
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                title = ""
                if title_match:
                    title = re.sub(r'<[^>]+>', '', title_match.group(1)).strip()
                    # ç§»é™¤CDATA
                    title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                
                # æå–æè¿°/æ‘˜è¦
                desc_patterns = [
                    r'<description[^>]*>(.*?)</description>',
                    r'<summary[^>]*>(.*?)</summary>',
                    r'<content[^>]*>(.*?)</content>'
                ]
                
                description = ""
                for pattern in desc_patterns:
                    desc_match = re.search(pattern, item, re.DOTALL | re.IGNORECASE)
                    if desc_match:
                        description = re.sub(r'<[^>]+>', '', desc_match.group(1)).strip()
                        description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                        break
                
                # æå–é“¾æ¥
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = ""
                if link_match:
                    link = link_match.group(1).strip()
                
                # æå–å‘å¸ƒæ—¶é—´
                date_patterns = [
                    r'<pubDate[^>]*>(.*?)</pubDate>',
                    r'<published[^>]*>(.*?)</published>',
                    r'<updated[^>]*>(.*?)</updated>'
                ]
                
                pub_date = datetime.now() - timedelta(hours=i)
                for pattern in date_patterns:
                    date_match = re.search(pattern, item, re.DOTALL | re.IGNORECASE)
                    if date_match:
                        try:
                            date_str = date_match.group(1).strip()
                            # å°è¯•è§£æå¸¸è§çš„æ—¥æœŸæ ¼å¼
                            pub_date = parse_rss_date(date_str)
                        except:
                            pass
                        break
                
                # åªæ·»åŠ æœ‰æ ‡é¢˜çš„æ–°é—»
                if title and len(title) > 10:
                    news_items.append({
                        'title': title[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        'summary': description[:300] if description else 'æš‚æ— æ‘˜è¦',
                        'link': link,
                        'source': source_name,
                        'published': pub_date
                    })
                    
            except Exception as e:
                log_debug(f"{source_name} - å¤„ç†item {i} å¤±è´¥: {str(e)}")
                continue
    
    except Exception as e:
        log_debug(f"{source_name} - RSSè§£æå¤±è´¥: {str(e)}")
    
    return news_items

def parse_rss_date(date_str):
    """è§£æRSSæ—¥æœŸ"""
    try:
        # ç§»é™¤å¸¸è§çš„æ—¶åŒºä¿¡æ¯
        date_str = re.sub(r'\s+[A-Z]{3,4}$', '', date_str)
        
        # å°è¯•å¸¸è§æ ¼å¼
        formats = [
            '%a, %d %b %Y %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d %H:%M:%S',
            '%d %b %Y %H:%M:%S'
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except:
                continue
                
        return datetime.now()
    except:
        return datetime.now()

def fetch_google_news_simple(query):
    """ç®€å•çš„Google Newsè·å–"""
    try:
        log_debug(f"å¼€å§‹è·å–Google News: {query}")
        
        # Google News RSS URL
        encoded_query = quote(query)
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        return test_basic_rss(url, "Google News")
        
    except Exception as e:
        log_debug(f"Google News è·å–å¤±è´¥: {str(e)}")
        return []

def get_all_news(ticker=None):
    """è·å–æ‰€æœ‰æ–°é—»æº"""
    all_news = []
    
    # RSSæ–°é—»æºåˆ—è¡¨
    rss_sources = [
        {
            'name': 'Yahoo Finance',
            'url': 'https://feeds.finance.yahoo.com/rss/2.0/headline?region=US&lang=en-US'
        },
        {
            'name': 'Reuters Business', 
            'url': 'http://feeds.reuters.com/reuters/businessNews'
        },
        {
            'name': 'MarketWatch',
            'url': 'https://feeds.marketwatch.com/marketwatch/topstories/'
        },
        {
            'name': 'CNN Money',
            'url': 'http://rss.cnn.com/rss/money_latest.rss'
        }
    ]
    
    # å¦‚æœæœ‰è‚¡ç¥¨ä»£ç ï¼Œä¹Ÿè·å–ç‰¹å®šæ–°é—»
    if ticker:
        # å°è¯•ç‰¹å®šè‚¡ç¥¨çš„Yahoo Finance RSS
        ticker_rss = {
            'name': f'Yahoo Finance {ticker}',
            'url': f'https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US'
        }
        rss_sources.insert(0, ticker_rss)
    
    # è·å–RSSæ–°é—»
    for source in rss_sources:
        news = test_basic_rss(source['url'], source['name'])
        all_news.extend(news)
    
    # è·å–Google News
    if ticker:
        google_query = f"{ticker} stock"
    else:
        google_query = "stock market news"
    
    google_news = fetch_google_news_simple(google_query)
    all_news.extend(google_news)
    
    # æŒ‰æ—¶é—´æ’åº
    all_news.sort(key=lambda x: x['published'], reverse=True)
    
    return all_news

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ”§ æ–°é—»è·å–æµ‹è¯•")
    
    ticker = st.text_input("è‚¡ç¥¨ä»£ç  (å¯é€‰):", placeholder="ä¾‹å¦‚: AAPL").upper().strip()
    
    st.markdown("---")
    
    if st.button("ğŸ“° å¼€å§‹è·å–æ–°é—»", type="primary"):
        st.session_state.debug_logs = []  # æ¸…ç©ºè°ƒè¯•æ—¥å¿—
        
        with st.spinner("æ­£åœ¨è·å–æ–°é—»..."):
            news_results = get_all_news(ticker)
            st.session_state.news_results = news_results
    
    if st.button("ğŸ”„ æ¸…é™¤ç»“æœ"):
        st.session_state.news_results = None
        st.session_state.debug_logs = []
    
    st.markdown("---")
    
    # æ˜¾ç¤ºè°ƒè¯•æ—¥å¿—
    st.subheader("ğŸ” è°ƒè¯•æ—¥å¿—")
    if st.session_state.debug_logs:
        for log in st.session_state.debug_logs[-20:]:  # åªæ˜¾ç¤ºæœ€å20æ¡
            st.text(log)
    else:
        st.text("æš‚æ— è°ƒè¯•ä¿¡æ¯")

# ä¸»ç•Œé¢
if st.session_state.news_results is not None:
    news_results = st.session_state.news_results
    
    if len(news_results) > 0:
        st.success(f"âœ… æˆåŠŸè·å– {len(news_results)} æ¡æ–°é—»ï¼")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        sources = {}
        for news in news_results:
            source = news['source']
            sources[source] = sources.get(source, 0) + 1
        
        st.markdown("### ğŸ“Š æ•°æ®æºç»Ÿè®¡")
        cols = st.columns(len(sources))
        for i, (source, count) in enumerate(sources.items()):
            with cols[i]:
                st.metric(source, count)
        
        st.markdown("---")
        
        # æ˜¾ç¤ºæ–°é—»åˆ—è¡¨
        st.markdown("### ğŸ“° æ–°é—»åˆ—è¡¨")
        
        for i, news in enumerate(news_results):
            with st.container():
                st.markdown(f"#### {i+1}. {news['title']}")
                
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.write(news['summary'])
                    if news['link']:
                        st.markdown(f"ğŸ”— [é˜…è¯»åŸæ–‡]({news['link']})")
                
                with col2:
                    st.write(f"**æ¥æº:** {news['source']}")
                    st.write(f"**æ—¶é—´:** {news['published'].strftime('%Y-%m-%d %H:%M')}")
                
                st.markdown("---")
    
    else:
        st.error("âŒ æœªè·å–åˆ°ä»»ä½•æ–°é—»")
        
        st.markdown("### ğŸ” è°ƒè¯•ä¿¡æ¯")
        if st.session_state.debug_logs:
            for log in st.session_state.debug_logs:
                st.text(log)

else:
    st.markdown("""
    ## ğŸ¯ ç®€å•æ–°é—»è·å–å™¨
    
    è¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äº**è°ƒè¯•å’Œæµ‹è¯•**çš„æ–°é—»è·å–ç³»ç»Ÿï¼š
    
    ### ğŸ“¡ æµ‹è¯•çš„æ–°é—»æº
    
    1. **Yahoo Finance RSS** - ä¸»è¦è´¢ç»æ–°é—»
    2. **Reuters Business** - è·¯é€ç¤¾å•†ä¸šæ–°é—»  
    3. **MarketWatch** - å¸‚åœºè§‚å¯Ÿæ–°é—»
    4. **CNN Money** - CNNè´¢ç»æ–°é—»
    5. **Google News** - è°·æ­Œæ–°é—»æœç´¢
    
    ### ğŸ”§ è°ƒè¯•åŠŸèƒ½
    
    - âœ… **è¯¦ç»†æ—¥å¿—** - æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„æ‰§è¡Œæƒ…å†µ
    - ğŸ“Š **çŠ¶æ€ç æ£€æŸ¥** - éªŒè¯HTTPè¯·æ±‚çŠ¶æ€
    - ğŸ” **å†…å®¹éªŒè¯** - æ£€æŸ¥RSSæ ¼å¼æœ‰æ•ˆæ€§
    - ğŸ“ˆ **ç»Ÿè®¡ä¿¡æ¯** - æ˜¾ç¤ºå„æºè·å–çš„æ–°é—»æ•°é‡
    
    ### ğŸš€ ä½¿ç”¨æ–¹æ³•
    
    1. **å¯é€‰è¾“å…¥è‚¡ç¥¨ä»£ç ** - è·å–ç‰¹å®šè‚¡ç¥¨æ–°é—»
    2. **ç‚¹å‡»"å¼€å§‹è·å–æ–°é—»"** - æµ‹è¯•æ‰€æœ‰æ–°é—»æº
    3. **æŸ¥çœ‹è°ƒè¯•æ—¥å¿—** - äº†è§£å…·ä½“çš„æ‰§è¡Œè¿‡ç¨‹
    4. **æ£€æŸ¥ç»“æœ** - æŸ¥çœ‹æˆåŠŸè·å–çš„æ–°é—»
    
    ---
    
    **ğŸ‘ˆ åœ¨å·¦ä¾§å¼€å§‹æµ‹è¯•æ–°é—»è·å–åŠŸèƒ½**
    
    è¿™ä¸ªç‰ˆæœ¬ä¸“æ³¨äº**è¯Šæ–­é—®é¢˜**ï¼Œä¼šæ˜¾ç¤ºè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯å¸®åŠ©å‘ç°è·å–å¤±è´¥çš„å…·ä½“åŸå› ã€‚
    """)

# é¡µè„š
st.markdown("---")
st.markdown("ğŸ“° ç®€å•æ–°é—»è·å–å™¨ | ğŸ”§ è°ƒè¯•ä¼˜å…ˆ | ğŸ“Š è¯¦ç»†æ—¥å¿— | ğŸ¯ é—®é¢˜è¯Šæ–­")
