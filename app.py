import streamlit as st
import yfinance as yf
import requests
from datetime import datetime, timedelta
import re
from urllib.parse import quote
import warnings
warnings.filterwarnings('ignore')

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ",
    page_icon="ğŸ“°",
    layout="wide"
)

st.title("ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ")
st.markdown("**åªä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„æ–°é—»æº - é«˜ç¨³å®šæ€§ - é«˜è´¨é‡æ–°é—»**")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'news_data' not in st.session_state:
    st.session_state.news_data = None
if 'source_stats' not in st.session_state:
    st.session_state.source_stats = {}

# ==================== æ–°é—»æº1: yfinanceï¼ˆå·²éªŒè¯100%æœ‰æ•ˆï¼‰====================
def get_yfinance_news(ticker, debug=False):
    """è·å–yfinanceæ–°é—» - å·²éªŒè¯æœ‰æ•ˆ"""
    try:
        if debug:
            st.sidebar.write(f"ğŸ” æ­£åœ¨è·å– yfinance {ticker} æ–°é—»...")
        
        stock = yf.Ticker(ticker)
        raw_news = stock.news
        
        if not raw_news:
            if debug:
                st.sidebar.warning("âš ï¸ yfinance: æ— æ–°é—»æ•°æ®")
            return []
        
        processed_news = []
        for i, article in enumerate(raw_news):
            try:
                if not isinstance(article, dict):
                    continue
                
                # å¤„ç†yfinanceçš„æ–°APIç»“æ„
                content_data = article.get('content', article)
                
                # å¤šç§æ–¹å¼æå–æ ‡é¢˜
                title = ''
                for title_field in ['title', 'headline', 'shortName']:
                    t = content_data.get(title_field, '') or article.get(title_field, '')
                    if t and len(str(t).strip()) > 10:
                        title = str(t).strip()
                        break
                
                if not title:
                    continue
                
                # å¤šç§æ–¹å¼æå–æ‘˜è¦
                summary = ''
                for summary_field in ['summary', 'description', 'snippet']:
                    s = content_data.get(summary_field, '') or article.get(summary_field, '')
                    if s and len(str(s).strip()) > 10:
                        summary = str(s).strip()
                        break
                
                # å¤šç§æ–¹å¼æå–URL
                url = ''
                # æ£€æŸ¥clickThroughUrl
                click_url = content_data.get('clickThroughUrl', {})
                if isinstance(click_url, dict):
                    url = click_url.get('url', '')
                elif isinstance(click_url, str):
                    url = click_url
                
                # å¤‡é€‰URLå­—æ®µ
                if not url:
                    for url_field in ['link', 'url', 'canonicalUrl']:
                        u = content_data.get(url_field, '') or article.get(url_field, '')
                        if u and isinstance(u, str) and len(u) > 10:
                            url = u
                            break
                
                # æå–æ—¶é—´
                published_time = datetime.now() - timedelta(hours=i+1)
                for time_field in ['providerPublishTime', 'publishedAt']:
                    time_val = content_data.get(time_field) or article.get(time_field)
                    if time_val:
                        try:
                            if isinstance(time_val, (int, float)):
                                published_time = datetime.fromtimestamp(time_val)
                                break
                            elif isinstance(time_val, str):
                                published_time = datetime.fromisoformat(time_val.replace('Z', '+00:00')).replace(tzinfo=None)
                                break
                        except:
                            continue
                
                processed_news.append({
                    'title': title,
                    'summary': summary or 'æ¥è‡ªYahoo Financeçš„è´¢ç»æ–°é—»',
                    'url': url,
                    'source': 'Yahoo Finance',
                    'published': published_time,
                    'method': 'yfinance'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"yfinanceå¤„ç†ç¬¬{i+1}æ¡æ–°é—»å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… yfinance: æˆåŠŸè·å– {len(processed_news)} æ¡æ–°é—»")
        
        return processed_news
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ yfinanceè·å–å¤±è´¥: {str(e)}")
        return []

# ==================== æ–°é—»æº2: Google Newsï¼ˆå¢å¼ºç‰ˆï¼‰====================
def get_google_news(query, debug=False):
    """è·å–Google News - å¢å¼ºç‰ˆ"""
    try:
        if debug:
            st.sidebar.write(f"ğŸ” æ­£åœ¨è·å–Google News: {query}")
        
        encoded_query = quote(query)
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, timeout=15, headers=headers)
        
        if response.status_code != 200:
            if debug:
                st.sidebar.warning(f"âš ï¸ Google News: HTTP {response.status_code}")
            return []
        
        content = response.text
        
        # æå–æ–°é—»é¡¹ç›®
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        if debug:
            st.sidebar.write(f"ğŸ“Š Google News: æ‰¾åˆ° {len(items)} ä¸ªæ–°é—»é¡¹ç›®")
        
        news_items = []
        for i, item in enumerate(items[:20]):  # å–å‰20ä¸ªï¼Œç„¶åç­›é€‰
            try:
                # æå–æ ‡é¢˜
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                if not title_match:
                    continue
                    
                title = title_match.group(1).strip()
                # å¤„ç†CDATAå’ŒHTML
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title)
                title = title.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '"')
                title = title.strip()
                
                if not title or len(title) < 15:
                    continue
                
                # æå–é“¾æ¥
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = link_match.group(1).strip() if link_match else ''
                
                # æå–å‘å¸ƒæ—¶é—´
                pub_date = datetime.now() - timedelta(hours=i/2)  # æ›´åˆç†çš„æ—¶é—´é—´éš”
                date_match = re.search(r'<pubDate[^>]*>(.*?)</pubDate>', item, re.DOTALL | re.IGNORECASE)
                if date_match:
                    try:
                        date_str = date_match.group(1).strip()
                        # ç§»é™¤æ—¶åŒºä¿¡æ¯
                        date_str = re.sub(r'\s+[A-Z]{3,4}$', '', date_str)
                        # å°è¯•è§£ææ—¥æœŸ
                        pub_date = datetime.strptime(date_str.strip(), '%a, %d %b %Y %H:%M:%S')
                    except:
                        pass
                
                news_items.append({
                    'title': title,
                    'summary': f'æ¥è‡ªGoogle Newsçš„{query}ç›¸å…³æ–°é—»æŠ¥é“',
                    'url': link,
                    'source': 'Google News',
                    'published': pub_date,
                    'method': 'Google News RSS'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"Google Newså¤„ç†ç¬¬{i+1}æ¡å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… Google News: æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ Google Newsè·å–å¤±è´¥: {str(e)}")
        return []

# ==================== æ–°é—»æº3: Yahoo Finance RSSï¼ˆå¤‡ç”¨æºï¼‰====================
def get_yahoo_rss_news(ticker=None, debug=False):
    """è·å–Yahoo Finance RSSæ–°é—»"""
    try:
        if debug:
            st.sidebar.write("ğŸ” æ­£åœ¨è·å–Yahoo Finance RSS...")
        
        url = "https://feeds.finance.yahoo.com/rss/2.0/headline?region=US&lang=en-US"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, timeout=15, headers=headers)
        
        if response.status_code != 200:
            if debug:
                st.sidebar.warning(f"âš ï¸ Yahoo RSS: HTTP {response.status_code}")
            return []
        
        content = response.text
        
        # ç®€å•çš„RSSè§£æ
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        if debug:
            st.sidebar.write(f"ğŸ“Š Yahoo RSS: æ‰¾åˆ° {len(items)} ä¸ªæ–°é—»é¡¹ç›®")
        
        news_items = []
        for i, item in enumerate(items[:8]):  # å–å‰8æ¡
            try:
                # æå–æ ‡é¢˜
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                if not title_match:
                    continue
                    
                title = title_match.group(1).strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title)
                title = title.replace('&amp;', '&').strip()
                
                if not title or len(title) < 10:
                    continue
                
                # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œç®€å•è¿‡æ»¤
                if ticker:
                    if (ticker.lower() not in title.lower() and 
                        ticker.lower() not in item.lower()):
                        continue
                
                # æå–é“¾æ¥
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = link_match.group(1).strip() if link_match else ''
                
                # æå–æè¿°
                desc_match = re.search(r'<description[^>]*>(.*?)</description>', item, re.DOTALL | re.IGNORECASE)
                description = ''
                if desc_match:
                    description = desc_match.group(1).strip()
                    description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                    description = re.sub(r'<[^>]+>', '', description)
                    description = description.replace('&amp;', '&').strip()
                
                news_items.append({
                    'title': title,
                    'summary': description[:200] if description else 'æ¥è‡ªYahoo Finance RSSçš„è´¢ç»æ–°é—»',
                    'url': link,
                    'source': 'Yahoo Finance RSS',
                    'published': datetime.now() - timedelta(hours=i/2),
                    'method': 'RSS'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"Yahoo RSSå¤„ç†ç¬¬{i+1}æ¡å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… Yahoo RSS: æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ Yahoo RSSè·å–å¤±è´¥: {str(e)}")
        return []

# ==================== å»é‡å’Œæ•´åˆç³»ç»Ÿ ====================
def smart_remove_duplicates(news_list):
    """æ™ºèƒ½å»é‡"""
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        # åˆ›å»ºæ ‡é¢˜æŒ‡çº¹ç”¨äºå»é‡
        title_fingerprint = re.sub(r'[^\w\s]', '', news['title'][:50].lower())
        title_fingerprint = re.sub(r'\s+', ' ', title_fingerprint).strip()
        
        if title_fingerprint not in seen_titles and len(title_fingerprint) > 10:
            seen_titles.add(title_fingerprint)
            unique_news.append(news)
    
    return unique_news

@st.cache_data(ttl=900)  # 15åˆ†é’Ÿç¼“å­˜
def get_all_reliable_news(ticker=None, debug=False):
    """è·å–æ‰€æœ‰å¯é æ–°é—»æºçš„æ–°é—»"""
    all_news = []
    source_stats = {}
    
    if debug:
        st.sidebar.markdown("### ğŸ” æ–°é—»è·å–è¿‡ç¨‹")
    
    # æº1: yfinanceï¼ˆä¸»åŠ›æºï¼‰
    if ticker:
        yf_news = get_yfinance_news(ticker, debug)
        all_news.extend(yf_news)
        source_stats['yfinance'] = len(yf_news)
    else:
        source_stats['yfinance'] = 0
    
    # æº2: Google Newsï¼ˆè¡¥å……æºï¼‰
    if ticker:
        google_query = f"{ticker} stock financial earnings revenue"
    else:
        google_query = "stock market financial news earnings revenue"
    
    google_news = get_google_news(google_query, debug)
    all_news.extend(google_news)
    source_stats['Google News'] = len(google_news)
    
    # æº3: Yahoo RSSï¼ˆå¤‡ç”¨æºï¼‰
    yahoo_rss_news = get_yahoo_rss_news(ticker, debug)
    all_news.extend(yahoo_rss_news)
    source_stats['Yahoo RSS'] = len(yahoo_rss_news)
    
    # æ™ºèƒ½å»é‡
    unique_news = smart_remove_duplicates(all_news)
    
    # æŒ‰æ—¶é—´æ’åº
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_before = len(all_news)
    total_after = len(unique_news)
    removed = total_before - total_after
    
    if debug:
        st.sidebar.info(f"ğŸ“Š åŸå§‹è·å–: {total_before} æ¡ï¼Œå»é‡å: {total_after} æ¡ï¼Œç§»é™¤é‡å¤: {removed} æ¡")
    
    return unique_news, source_stats

# ==================== ç¿»è¯‘å’Œåˆ†æåŠŸèƒ½ ====================
def translate_financial_content(text):
    """è´¢ç»å†…å®¹ç¿»è¯‘"""
    if not text:
        return text
    
    # æ‰©å±•çš„è´¢ç»æœ¯è¯­è¯å…¸
    financial_terms = {
        # åŸºç¡€æœ¯è¯­
        'earnings': 'è´¢æŠ¥', 'revenue': 'è¥æ”¶', 'profit': 'åˆ©æ¶¦', 'loss': 'äºæŸ',
        'stock': 'è‚¡ç¥¨', 'shares': 'è‚¡ä»·', 'market': 'å¸‚åœº', 'trading': 'äº¤æ˜“',
        
        # åŠ¨ä½œè¯æ±‡
        'announced': 'å®£å¸ƒ', 'reported': 'æŠ¥å‘Š', 'released': 'å‘å¸ƒ', 'disclosed': 'æŠ«éœ²',
        'increased': 'å¢é•¿', 'decreased': 'ä¸‹é™', 'rose': 'ä¸Šæ¶¨', 'fell': 'ä¸‹è·Œ',
        'gained': 'ä¸Šæ¶¨', 'dropped': 'ä¸‹è·Œ', 'surged': 'é£™å‡', 'plunged': 'æš´è·Œ',
        
        # ä¸šç»©è¯æ±‡
        'beat': 'è¶…è¿‡', 'missed': 'æœªè¾¾åˆ°', 'exceeded': 'è¶…è¿‡', 'outperformed': 'è¡¨ç°è¶…å‡º',
        'strong': 'å¼ºåŠ²', 'weak': 'ç–²è½¯', 'solid': 'ç¨³å¥', 'robust': 'å¼ºå¥',
        
        # æ—¶é—´è¯æ±‡
        'quarterly': 'å­£åº¦', 'annual': 'å¹´åº¦', 'monthly': 'æœˆåº¦',
        'year-over-year': 'åŒæ¯”', 'quarter-over-quarter': 'ç¯æ¯”',
        
        # æ•°é‡è¯æ±‡
        'billion': 'åäº¿', 'million': 'ç™¾ä¸‡', 'thousand': 'åƒ',
        'percent': 'ç™¾åˆ†æ¯”', 'percentage': 'ç™¾åˆ†æ¯”'
    }
    
    result = text
    for en, zh in financial_terms.items():
        pattern = r'\b' + re.escape(en) + r'\b'
        result = re.sub(pattern, zh, result, flags=re.IGNORECASE)
    
    # å¤„ç†æ•°å­—è¡¨è¾¾
    result = re.sub(r'\$([0-9,.]+)\s*billion', r'\1åäº¿ç¾å…ƒ', result, flags=re.IGNORECASE)
    result = re.sub(r'\$([0-9,.]+)\s*million', r'\1ç™¾ä¸‡ç¾å…ƒ', result, flags=re.IGNORECASE)
    result = re.sub(r'([0-9,.]+)%', r'\1%', result)
    
    return result

def analyze_news_sentiment(title, summary):
    """æ–°é—»æƒ…ç»ªåˆ†æ"""
    text = (title + ' ' + summary).lower()
    
    # ç§¯æè¯æ±‡
    positive_words = [
        'beat', 'strong', 'growth', 'increase', 'rise', 'gain', 'up', 'success', 
        'record', 'high', 'outperform', 'exceed', 'robust', 'solid', 'win'
    ]
    
    # æ¶ˆæè¯æ±‡
    negative_words = [
        'miss', 'weak', 'decline', 'fall', 'drop', 'down', 'loss', 'concern', 
        'worry', 'low', 'underperform', 'disappoint', 'struggle', 'challenge'
    ]
    
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    if pos_count > neg_count and pos_count > 0:
        return 'åˆ©å¥½', 'green'
    elif neg_count > pos_count and neg_count > 0:
        return 'åˆ©ç©º', 'red'
    else:
        return 'ä¸­æ€§', 'gray'

# ==================== ç”¨æˆ·ç•Œé¢ ====================
# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ“° å¯é æ–°é—»æºè®¾ç½®")
    
    ticker = st.text_input(
        "è‚¡ç¥¨ä»£ç  (å¯é€‰):",
        placeholder="ä¾‹å¦‚: AAPL, AMZN, TSLA, BTC",
        help="è¾“å…¥ä»£ç è·å–ç›¸å…³æ–°é—»ï¼Œç•™ç©ºè·å–å¸‚åœºç»¼åˆæ–°é—»"
    ).upper().strip()
    
    st.markdown("---")
    
    st.markdown("#### ğŸ“¡ å¯ç”¨çš„æ–°é—»æº")
    st.success("âœ… **yfinance** - é«˜è´¨é‡è´¢ç»æ–°é—»")
    st.success("âœ… **Google News** - å¹¿æ³›æ–°é—»èšåˆ")
    st.success("âœ… **Yahoo RSS** - ç¨³å®šå¤‡ç”¨æº")
    
    st.markdown("---")
    
    debug_mode = st.checkbox("ğŸ”§ æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯", help="æ˜¾ç¤ºè¯¦ç»†çš„è·å–è¿‡ç¨‹")
    show_translation = st.checkbox("ğŸŒ æ˜¾ç¤ºç¿»è¯‘", value=True, help="æ˜¾ç¤ºè´¢ç»æœ¯è¯­ç¿»è¯‘")
    
    st.markdown("---")
    
    if st.button("ğŸ“° è·å–å¯é æ–°é—»", type="primary"):
        with st.spinner("æ­£åœ¨ä»å¯é æ–°é—»æºè·å–æ•°æ®..."):
            news_data, stats = get_all_reliable_news(ticker, debug_mode)
            st.session_state.news_data = news_data
            st.session_state.source_stats = stats
    
    if st.button("ğŸ”„ æ¸…é™¤ç¼“å­˜"):
        get_all_reliable_news.clear()
        st.session_state.news_data = None
        st.session_state.source_stats = {}
        st.success("ç¼“å­˜å·²æ¸…é™¤ï¼")

# ä¸»ç•Œé¢
if st.session_state.news_data is not None:
    news_data = st.session_state.news_data
    source_stats = st.session_state.source_stats
    
    if len(news_data) > 0:
        # æ•°æ®æºç»Ÿè®¡é¢æ¿
        st.subheader("ğŸ“Š å¯é æ•°æ®æºç»Ÿè®¡")
        
        cols = st.columns(len(source_stats) + 1)
        
        total_unique = len(news_data)
        total_raw = sum(source_stats.values())
        
        with cols[0]:
            st.metric("ğŸ“° æœ€ç»ˆç»“æœ", f"{total_unique} æ¡", f"åŸå§‹: {total_raw}")
        
        for i, (source, count) in enumerate(source_stats.items(), 1):
            with cols[i]:
                if count > 0:
                    st.metric(source, f"{count} æ¡", delta="âœ…")
                else:
                    st.metric(source, f"{count} æ¡", delta="âŒ")
        
        # ç³»ç»Ÿå¯é æ€§è¯„ä¼°
        working_sources = len([count for count in source_stats.values() if count > 0])
        total_sources = len(source_stats)
        reliability = working_sources / total_sources * 100
        
        if reliability >= 80:
            st.success(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - ä¼˜ç§€")
        elif reliability >= 60:
            st.warning(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - è‰¯å¥½")
        else:
            st.error(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - éœ€è¦æ”¹è¿›")
        
        st.markdown("---")
        
        # æ–°é—»åˆ—è¡¨å±•ç¤º
        st.subheader(f"ğŸ“° {ticker or 'å¸‚åœº'} æœ€æ–°æ–°é—»")
        
        for i, news in enumerate(news_data):
            with st.container():
                # æƒ…ç»ªåˆ†æ
                sentiment, color = analyze_news_sentiment(news['title'], news['summary'])
                
                # æ ‡é¢˜ï¼ˆå¸¦ç¿»è¯‘ï¼‰
                title_display = news['title']
                if show_translation:
                    title_display = translate_financial_content(title_display)
                
                st.markdown(f"### {i+1}. {title_display}")
                
                # å…ƒä¿¡æ¯è¡Œ
                time_str = news['published'].strftime('%Y-%m-%d %H:%M')
                st.caption(f"ğŸ•’ {time_str} | ğŸ“¡ {news['source']} | ğŸ”§ {news['method']}")
                
                # ä¸»è¦å†…å®¹åŒºåŸŸ
                col_main, col_side = st.columns([3, 1])
                
                with col_main:
                    # æ‘˜è¦å†…å®¹ï¼ˆå¸¦ç¿»è¯‘ï¼‰
                    display_summary = news['summary']
                    if show_translation:
                        display_summary = translate_financial_content(display_summary)
                    st.write(display_summary)
                    
                    # åŸæ–‡é“¾æ¥
                    if news['url']:
                        st.markdown(f"ğŸ”— [é˜…è¯»åŸæ–‡]({news['url']})")
                
                with col_side:
                    # æƒ…ç»ªåˆ†ææ˜¾ç¤º
                    st.markdown(f"**æƒ…ç»ªåˆ†æ:**")
                    st.markdown(f"<span style='color:{color}; font-weight:bold; font-size:18px'>{sentiment}</span>", unsafe_allow_html=True)
                    
                    # æ¥æºè´¨é‡æ ‡è¯†
                    if news['method'] == 'yfinance':
                        st.write("ğŸ¥‡ **é«˜è´¨é‡æº**")
                    elif 'Google News' in news['method']:
                        st.write("ğŸ¥ˆ **èšåˆæº**")
                    else:
                        st.write("ğŸ¥‰ **è¡¥å……æº**")
                
                st.markdown("---")
        
        # åº•éƒ¨æƒ…ç»ªç»Ÿè®¡
        st.markdown("### ğŸ“ˆ æ•´ä½“å¸‚åœºæƒ…ç»ªåˆ†æ")
        sentiments = {}
        for news in news_data:
            sentiment, _ = analyze_news_sentiment(news['title'], news['summary'])
            sentiments[sentiment] = sentiments.get(sentiment, 0) + 1
        
        sentiment_cols = st.columns(3)
        for i, (sentiment, count) in enumerate(sentiments.items()):
            with sentiment_cols[i]:
                pct = count / len(news_data) * 100
                if sentiment == 'åˆ©å¥½':
                    st.success(f"ğŸ“ˆ **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
                elif sentiment == 'åˆ©ç©º':
                    st.error(f"ğŸ“‰ **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
                else:
                    st.info(f"ğŸ“Š **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
    
    else:
        st.warning("ğŸ“­ æœªè·å–åˆ°æ–°é—»æ•°æ®")
        
        if st.session_state.source_stats:
            st.markdown("### ğŸ“Š å„æºè·å–ç»“æœ:")
            for source, count in st.session_state.source_stats.items():
                if count > 0:
                    st.success(f"âœ… **{source}**: {count} æ¡")
                else:
                    st.error(f"âŒ **{source}**: {count} æ¡")

else:
    # æ¬¢è¿é¡µé¢
    st.markdown("""
    ## ğŸ¯ æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ
    
    ### ğŸ“¡ **åªä½¿ç”¨ç»è¿‡éªŒè¯çš„æ–°é—»æº**
    
    #### ğŸ¥‡ **yfinance (ä¸»åŠ›æº)**
    - âœ… **å·²éªŒè¯100%æœ‰æ•ˆ** - æ¯æ¬¡ç¨³å®šè·å–10æ¡é«˜è´¨é‡æ–°é—»
    - ğŸ“° **å†…å®¹ç²¾å‡†** - ç›´æ¥ç›¸å…³çš„è´¢ç»æ–°é—»
    - ğŸ”— **é“¾æ¥å®Œæ•´** - æ‰€æœ‰æ–°é—»éƒ½å¯ç‚¹å‡»é˜…è¯»åŸæ–‡
    - ğŸ“Š **ç»“æ„åŒ–æ•°æ®** - åŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€é“¾æ¥ã€æ—¶é—´
    
    #### ğŸ¥ˆ **Google News (è¡¥å……æº)**
    - âœ… **å¹¿æ³›èšåˆ** - æ±‡é›†å¤šä¸ªæƒå¨æ–°é—»æº
    - âš¡ **å®æ—¶æ›´æ–°** - æ–°é—»æ›´æ–°é¢‘ç‡å¾ˆé«˜
    - ğŸ” **æ™ºèƒ½æœç´¢** - æ ¹æ®è‚¡ç¥¨ä»£ç ç²¾å‡†åŒ¹é…
    - ğŸ“ˆ **æ•°é‡å¯è§‚** - é€šå¸¸æä¾›10-20æ¡ç›¸å…³æ–°é—»
    
    #### ğŸ¥‰ **Yahoo Finance RSS (å¤‡ç”¨æº)**
    - âœ… **å®˜æ–¹ç¨³å®š** - Yahooå®˜æ–¹RSSæœåŠ¡
    - ğŸ›¡ï¸ **é«˜å¯é æ€§** - æå°‘å‡ºç°è¿æ¥é—®é¢˜
    - ğŸ“° **è´¢ç»ä¸“ä¸š** - ä¸“æ³¨è´¢ç»é¢†åŸŸæ–°é—»
    - ğŸ”„ **è‡ªåŠ¨æ›´æ–°** - æŒç»­æä¾›æœ€æ–°å†…å®¹
    
    ### ğŸ›¡ï¸ **ç³»ç»Ÿä¼˜åŠ¿**
    
    #### ğŸ“Š **é«˜å¯é æ€§**
    - **å¤šé‡å¤‡ä»½**: 3ä¸ªç‹¬ç«‹æ–°é—»æº
    - **æ™ºèƒ½é™çº§**: å•ä¸ªæºå¤±æ•ˆä¸å½±å“æ•´ä½“
    - **å®æ—¶ç›‘æ§**: æ˜¾ç¤ºå„æºå·¥ä½œçŠ¶æ€
    - **å¯é æ€§è¯„åˆ†**: åŠ¨æ€è¯„ä¼°ç³»ç»Ÿå¥åº·åº¦
    
    #### ğŸ¯ **é«˜è´¨é‡å†…å®¹**
    - **æ™ºèƒ½å»é‡**: è‡ªåŠ¨è¿‡æ»¤é‡å¤æ–°é—»
    - **æ—¶é—´æ’åº**: æœ€æ–°æ–°é—»ä¼˜å…ˆæ˜¾ç¤º
    - **å®Œæ•´ä¿¡æ¯**: æ ‡é¢˜ã€æ‘˜è¦ã€é“¾æ¥ã€æ—¶é—´é½å…¨
    - **æƒ…ç»ªåˆ†æ**: AIåˆ¤æ–­æ–°é—»å¯¹å¸‚åœºçš„å½±å“
    
    #### ğŸŒ **ç”¨æˆ·å‹å¥½**
    - **è´¢ç»ç¿»è¯‘**: ä¸“ä¸šæœ¯è¯­ä¸­æ–‡åŒ–
    - **æ¸…æ™°åˆ†ç±»**: æŒ‰æ¥æºè´¨é‡åˆ†çº§æ˜¾ç¤º
    - **å“åº”å¿«é€Ÿ**: 15åˆ†é’Ÿæ™ºèƒ½ç¼“å­˜
    - **è°ƒè¯•é€æ˜**: è¯¦ç»†çš„è·å–è¿‡ç¨‹æ—¥å¿—
    
    ### ğŸ’¡ **é¢„æœŸæ•ˆæœ**
    
    ä½¿ç”¨æœ¬ç³»ç»Ÿï¼Œä½ é€šå¸¸å¯ä»¥è·å¾—ï¼š
    - ğŸ¥‡ **yfinance**: 10æ¡ç²¾å‡†è´¢ç»æ–°é—»
    - ğŸ¥ˆ **Google News**: 10-20æ¡èšåˆæ–°é—»
    - ğŸ¥‰ **Yahoo RSS**: 3-8æ¡è¡¥å……æ–°é—»
    - **ğŸ“Š æ€»è®¡**: 25-40æ¡ä¼˜è´¨å»é‡æ–°é—»
    - **ğŸ›¡ï¸ å¯é æ€§**: 90%ä»¥ä¸Šçš„æˆåŠŸç‡
    
    ### ğŸš€ **ç«‹å³å¼€å§‹**
    
    1. åœ¨å·¦ä¾§è¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆå¦‚AAPLã€AMZNã€TSLAï¼‰
    2. æˆ–ç•™ç©ºè·å–å¸‚åœºç»¼åˆæ–°é—»
    3. ç‚¹å‡»"è·å–å¯é æ–°é—»"
    4. äº«å—é«˜è´¨é‡çš„è´¢ç»æ–°é—»ä½“éªŒ
    
    ---
    
    **ğŸ‘ˆ åœ¨å·¦ä¾§å¼€å§‹ä½¿ç”¨æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ**
    """)

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ | âœ… åªä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„æº | ğŸ›¡ï¸ 90%+ å¯é æ€§ | ğŸ“Š 25-40æ¡ä¼˜è´¨æ–°é—»
</div>
""", unsafe_allow_html=True)
