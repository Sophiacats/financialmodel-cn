import streamlit as st
import yfinance as yf
import requests
from datetime import datetime, timedelta
import re
from urllib.parse import quote
import warnings
import hashlib
import time
import json
warnings.filterwarnings('ignore')

# ==================== äº‘ç¿»è¯‘APIæ–¹æ¡ˆï¼ˆå®¢æˆ·æ— éœ€å®‰è£…ï¼‰====================

def translate_with_free_api(text: str, target_lang: str = 'zh') -> str:
    """ä½¿ç”¨å…è´¹ç¿»è¯‘APIæœåŠ¡"""
    if not text or len(text.strip()) < 3:
        return text
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«ä¸­æ–‡
    if any('\u4e00' <= char <= '\u9fff' for char in text):
        return text
    
    try:
        # æ–¹æ¡ˆ1: ä½¿ç”¨MyMemoryå…è´¹ç¿»è¯‘API (æ¯å¤©å…è´¹1000æ¬¡è°ƒç”¨)
        url = "https://api.mymemory.translated.net/get"
        params = {
            'q': text[:500],  # é™åˆ¶é•¿åº¦
            'langpair': f'en|{target_lang}'
        }
        
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            result = response.json()
            if result.get('responseStatus') == 200:
                translated = result['responseData']['translatedText']
                if translated and translated != text:
                    return translated
    except:
        pass
    
    try:
        # æ–¹æ¡ˆ2: ä½¿ç”¨Libretranslateå…è´¹API
        url = "https://libretranslate.de/translate"
        data = {
            'q': text[:500],
            'source': 'en',
            'target': 'zh',
            'format': 'text'
        }
        
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            result = response.json()
            translated = result.get('translatedText', '')
            if translated and translated != text:
                return translated
    except:
        pass
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šé«˜çº§è¯å…¸ç¿»è¯‘
    return advanced_financial_translate(text)

def translate_with_baidu_api(text: str, app_id: str = None, secret_key: str = None) -> str:
    """ç™¾åº¦ç¿»è¯‘APIï¼ˆå¯é€‰ï¼Œéœ€è¦API keyï¼‰"""
    if not app_id or not secret_key:
        return None
    
    try:
        url = 'https://fanyi-api.baidu.com/api/trans/vip/translate'
        salt = str(int(time.time()))
        sign_str = app_id + text + salt + secret_key
        sign = hashlib.md5(sign_str.encode()).hexdigest()
        
        params = {
            'q': text,
            'from': 'en',
            'to': 'zh',
            'appid': app_id,
            'salt': salt,
            'sign': sign
        }
        
        response = requests.get(url, params=params, timeout=10)
        result = response.json()
        
        if 'trans_result' in result:
            translations = [item['dst'] for item in result['trans_result']]
            return ' '.join(translations)
    except:
        pass
    
    return None

@st.cache_data(ttl=3600)  # ç¿»è¯‘ç»“æœç¼“å­˜1å°æ—¶
def smart_translate_text(text: str, use_api: bool = True, api_config: dict = None) -> str:
    """æ™ºèƒ½ç¿»è¯‘æ–‡æœ¬"""
    if not text or len(text.strip()) < 3:
        return text
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«ä¸­æ–‡
    if any('\u4e00' <= char <= '\u9fff' for char in text):
        return text
    
    if use_api:
        # ä¼˜å…ˆä½¿ç”¨ä»˜è´¹APIï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if api_config and api_config.get('baidu_app_id'):
            baidu_result = translate_with_baidu_api(
                text, 
                api_config['baidu_app_id'], 
                api_config['baidu_secret_key']
            )
            if baidu_result:
                return baidu_result
        
        # ä½¿ç”¨å…è´¹API
        free_result = translate_with_free_api(text)
        if free_result != text:  # å¦‚æœç¿»è¯‘æˆåŠŸ
            return free_result
    
    # æœ€ç»ˆå¤‡ç”¨ï¼šé«˜çº§è¯å…¸ç¿»è¯‘
    return advanced_financial_translate(text)

def advanced_financial_translate(text: str) -> str:
    """é«˜çº§è´¢ç»è¯å…¸ç¿»è¯‘ç³»ç»Ÿï¼ˆæ— éœ€å¤–éƒ¨ä¾èµ–ï¼‰"""
    if not text or len(text.strip()) < 3:
        return text
    
    # è¶…å¤§è´¢ç»è¯å…¸
    translations = {
        # å…¬å¸ç›¸å…³
        'company': 'å…¬å¸', 'corporation': 'å…¬å¸', 'inc': 'å…¬å¸', 'ltd': 'æœ‰é™å…¬å¸',
        'group': 'é›†å›¢', 'holdings': 'æ§è‚¡', 'enterprises': 'ä¼ä¸š', 'industries': 'å®ä¸š',
        'technologies': 'ç§‘æŠ€', 'systems': 'ç³»ç»Ÿ', 'solutions': 'è§£å†³æ–¹æ¡ˆ',
        'services': 'æœåŠ¡', 'international': 'å›½é™…', 'global': 'å…¨çƒ',
        
        # åŸºç¡€è´¢åŠ¡æœ¯è¯­
        'earnings': 'è´¢æŠ¥', 'revenue': 'è¥æ”¶', 'sales': 'é”€å”®é¢', 'income': 'æ”¶å…¥',
        'profit': 'åˆ©æ¶¦', 'loss': 'äºæŸ', 'net income': 'å‡€æ”¶å…¥', 'gross profit': 'æ¯›åˆ©æ¶¦',
        'operating income': 'è¥ä¸šæ”¶å…¥', 'ebitda': 'EBITDA', 'cash flow': 'ç°é‡‘æµ',
        'expenses': 'æ”¯å‡º', 'costs': 'æˆæœ¬', 'margin': 'åˆ©æ¶¦ç‡', 'growth': 'å¢é•¿',
        
        # è‚¡ç¥¨å¸‚åœº
        'stock': 'è‚¡ç¥¨', 'shares': 'è‚¡ä»½', 'share price': 'è‚¡ä»·', 'market cap': 'å¸‚å€¼',
        'trading': 'äº¤æ˜“', 'volume': 'æˆäº¤é‡', 'market': 'å¸‚åœº', 'exchange': 'äº¤æ˜“æ‰€',
        'nasdaq': 'çº³æ–¯è¾¾å…‹', 'nyse': 'çº½äº¤æ‰€', 'dow jones': 'é“ç¼æ–¯', 's&p 500': 'æ ‡æ™®500',
        
        # æŠ•èµ„è€…ç›¸å…³
        'investors': 'æŠ•èµ„è€…', 'shareholders': 'è‚¡ä¸œ', 'analyst': 'åˆ†æå¸ˆ', 'analysts': 'åˆ†æå¸ˆä»¬',
        'institutional investors': 'æœºæ„æŠ•èµ„è€…', 'retail investors': 'æ•£æˆ·æŠ•èµ„è€…',
        'portfolio': 'æŠ•èµ„ç»„åˆ', 'fund': 'åŸºé‡‘', 'hedge fund': 'å¯¹å†²åŸºé‡‘',
        
        # ä¸šç»©è¡¨ç°
        'beat': 'è¶…å‡ºé¢„æœŸ', 'missed': 'æœªè¾¾é¢„æœŸ', 'exceeded': 'è¶…è¿‡', 'outperformed': 'è¡¨ç°è¶…å‡º',
        'underperformed': 'è¡¨ç°ä¸ä½³', 'met': 'ç¬¦åˆé¢„æœŸ', 'expectations': 'é¢„æœŸ',
        'estimates': 'é¢„ä¼°', 'consensus': 'ä¸€è‡´é¢„æœŸ', 'guidance': 'æŒ‡å¼•',
        
        # æƒ…ç»ªè¯æ±‡
        'strong': 'å¼ºåŠ²', 'weak': 'ç–²è½¯', 'solid': 'ç¨³å¥', 'robust': 'å¼ºå¥',
        'disappointing': 'ä»¤äººå¤±æœ›', 'impressive': 'ä»¤äººå°è±¡æ·±åˆ»', 'outstanding': 'æ°å‡º',
        'volatile': 'æ³¢åŠ¨', 'stable': 'ç¨³å®š', 'uncertain': 'ä¸ç¡®å®š', 'optimistic': 'ä¹è§‚',
        
        # åŠ¨ä½œè¯æ±‡
        'announced': 'å®£å¸ƒ', 'reported': 'æŠ¥å‘Š', 'released': 'å‘å¸ƒ', 'disclosed': 'æŠ«éœ²',
        'launched': 'æ¨å‡º', 'introduced': 'æ¨ä»‹', 'unveiled': 'æ­æ™“', 'presented': 'å±•ç¤º',
        'acquired': 'æ”¶è´­', 'merged': 'åˆå¹¶', 'divested': 'å‰¥ç¦»', 'spun off': 'åˆ†æ‹†',
        
        # ä»·æ ¼å˜åŠ¨
        'increased': 'å¢é•¿', 'decreased': 'ä¸‹é™', 'rose': 'ä¸Šæ¶¨', 'fell': 'ä¸‹è·Œ',
        'gained': 'ä¸Šæ¶¨', 'dropped': 'ä¸‹è·Œ', 'surged': 'é£™å‡', 'plunged': 'æš´è·Œ',
        'rallied': 'åå¼¹', 'tumbled': 'é‡æŒ«', 'soared': 'é£™å‡', 'crashed': 'æš´è·Œ',
        'climbed': 'æ”€å‡', 'slipped': 'ä¸‹æ»‘', 'jumped': 'è·³æ¶¨', 'dipped': 'ä¸‹è·Œ',
        
        # æ—¶é—´è¡¨è¾¾
        'quarterly': 'å­£åº¦', 'annual': 'å¹´åº¦', 'monthly': 'æœˆåº¦', 'daily': 'æ—¥åº¦',
        'year-over-year': 'åŒæ¯”', 'quarter-over-quarter': 'ç¯æ¯”', 'yoy': 'åŒæ¯”',
        'q1': 'ç¬¬ä¸€å­£åº¦', 'q2': 'ç¬¬äºŒå­£åº¦', 'q3': 'ç¬¬ä¸‰å­£åº¦', 'q4': 'ç¬¬å››å­£åº¦',
        
        # æ•°é‡å•ä½
        'billion': 'åäº¿', 'million': 'ç™¾ä¸‡', 'thousand': 'åƒ', 'trillion': 'ä¸‡äº¿',
        'percent': 'ç™¾åˆ†æ¯”', 'percentage': 'ç™¾åˆ†æ¯”', 'basis points': 'åŸºç‚¹',
        'dollars': 'ç¾å…ƒ', 'cents': 'ç¾åˆ†',
        
        # è¡Œä¸šæœ¯è¯­
        'ipo': 'IPO/é¦–æ¬¡å…¬å¼€å‹Ÿè‚¡', 'buyback': 'è‚¡ç¥¨å›è´­', 'dividend': 'è‚¡æ¯åˆ†çº¢',
        'split': 'è‚¡ç¥¨åˆ†å‰²', 'merger': 'å¹¶è´­', 'acquisition': 'æ”¶è´­',
        'partnership': 'åˆä½œä¼™ä¼´å…³ç³»', 'joint venture': 'åˆèµ„ä¼ä¸š',
        'forecast': 'é¢„æµ‹', 'outlook': 'å‰æ™¯å±•æœ›', 'projections': 'é¢„æµ‹',
        
        # è¯„çº§ç›¸å…³
        'rating': 'è¯„çº§', 'upgrade': 'ä¸Šè°ƒè¯„çº§', 'downgrade': 'ä¸‹è°ƒè¯„çº§',
        'buy': 'ä¹°å…¥', 'sell': 'å–å‡º', 'hold': 'æŒæœ‰', 'overweight': 'è¶…é…',
        'underweight': 'ä½é…', 'neutral': 'ä¸­æ€§', 'target price': 'ç›®æ ‡ä»·',
        
        # æŠ€æœ¯åˆ†æ
        'support': 'æ”¯æ’‘ä½', 'resistance': 'é˜»åŠ›ä½', 'trend': 'è¶‹åŠ¿',
        'bullish': 'çœ‹æ¶¨', 'bearish': 'çœ‹è·Œ', 'momentum': 'åŠ¨é‡',
        'volatility': 'æ³¢åŠ¨æ€§', 'liquidity': 'æµåŠ¨æ€§',
        
        # å®è§‚ç»æµ
        'inflation': 'é€šèƒ€', 'recession': 'è¡°é€€', 'recovery': 'å¤è‹',
        'interest rates': 'åˆ©ç‡', 'fed': 'ç¾è”å‚¨', 'federal reserve': 'ç¾è”å‚¨',
        'gdp': 'GDP/å›½å†…ç”Ÿäº§æ€»å€¼', 'unemployment': 'å¤±ä¸šç‡',
        
        # è¡Œä¸šæ¿å—
        'technology': 'ç§‘æŠ€', 'healthcare': 'åŒ»ç–—', 'finance': 'é‡‘è',
        'energy': 'èƒ½æº', 'utilities': 'å…¬ç”¨äº‹ä¸š', 'consumer': 'æ¶ˆè´¹',
        'industrial': 'å·¥ä¸š', 'materials': 'ææ–™', 'real estate': 'æˆ¿åœ°äº§',
        
        # æ–°é—»åŠ¨ä½œ
        'according to': 'æ®...ç§°', 'sources said': 'æ¶ˆæ¯äººå£«é€éœ²',
        'reported that': 'æŠ¥é“ç§°', 'confirmed': 'ç¡®è®¤', 'denied': 'å¦è®¤',
        'estimated': 'ä¼°è®¡', 'projected': 'é¢„è®¡', 'expected': 'é¢„æœŸ',
        
        # å¸¸ç”¨è¿æ¥è¯
        'however': 'ç„¶è€Œ', 'meanwhile': 'ä¸æ­¤åŒæ—¶', 'additionally': 'æ­¤å¤–',
        'furthermore': 'æ­¤å¤–', 'nevertheless': 'ç„¶è€Œ', 'moreover': 'æ­¤å¤–',
        'therefore': 'å› æ­¤', 'consequently': 'å› æ­¤', 'as a result': 'å› æ­¤',
        
        # æ¯”è¾ƒè¯æ±‡
        'compared to': 'ä¸...ç›¸æ¯”', 'versus': 'å¯¹æ¯”', 'higher than': 'é«˜äº',
        'lower than': 'ä½äº', 'better than': 'ä¼˜äº', 'worse than': 'é€Šäº',
        
        # ç‰¹æ®Šè¡¨è¾¾
        'all-time high': 'å†å²æ–°é«˜', 'all-time low': 'å†å²æ–°ä½',
        'record high': 'åˆ›çºªå½•é«˜ä½', 'record low': 'åˆ›çºªå½•ä½ä½',
        'year-to-date': 'å¹´åˆè‡³ä»Š', 'month-to-date': 'æœˆåˆè‡³ä»Š'
    }
    
    result = text
    
    # é¦–å…ˆå¤„ç†å¤åˆè¯ç»„
    compound_phrases = {
        'all-time high': 'å†å²æ–°é«˜', 'all-time low': 'å†å²æ–°ä½', 
        'record high': 'åˆ›çºªå½•é«˜ä½', 'record low': 'åˆ›çºªå½•ä½ä½',
        'year-over-year': 'åŒæ¯”', 'quarter-over-quarter': 'ç¯æ¯”',
        'net income': 'å‡€æ”¶å…¥', 'gross profit': 'æ¯›åˆ©æ¶¦',
        'operating income': 'è¥ä¸šæ”¶å…¥', 'cash flow': 'ç°é‡‘æµ',
        'market cap': 'å¸‚å€¼', 'share price': 'è‚¡ä»·'
    }
    
    # å…ˆå¤„ç†çŸ­è¯­
    for en_phrase, zh_phrase in compound_phrases.items():
        pattern = r'\b' + re.escape(en_phrase) + r'\b'
        result = re.sub(pattern, f"{en_phrase}({zh_phrase})", result, flags=re.IGNORECASE)
    
    # å†å¤„ç†å•è¯
    for en_word, zh_word in translations.items():
        if en_word not in compound_phrases:
            pattern = r'\b' + re.escape(en_word) + r'\b'
            result = re.sub(pattern, f"{en_word}({zh_word})", result, flags=re.IGNORECASE)
    
    # å¤„ç†æ•°å­—è¡¨è¾¾
    result = re.sub(r'\$([0-9,.]+)\s*billion', r'\1åäº¿ç¾å…ƒ', result, flags=re.IGNORECASE)
    result = re.sub(r'\$([0-9,.]+)\s*million', r'\1ç™¾ä¸‡ç¾å…ƒ', result, flags=re.IGNORECASE)
    result = re.sub(r'\$([0-9,.]+)\s*thousand', r'\1åƒç¾å…ƒ', result, flags=re.IGNORECASE)
    result = re.sub(r'\$([0-9,.]+)', r'\1ç¾å…ƒ', result)
    result = re.sub(r'([0-9,.]+)%', r'\1%', result)
    
    return result

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (äº‘ç¿»è¯‘ç‰ˆ)",
    page_icon="ğŸ“°",
    layout="wide"
)

st.title("ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (äº‘ç¿»è¯‘ç‰ˆ)")
st.markdown("**åªä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„æ–°é—»æº - é«˜ç¨³å®šæ€§ - é«˜è´¨é‡æ–°é—» - ğŸŒ äº‘ç«¯æ™ºèƒ½ç¿»è¯‘**")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'news_data' not in st.session_state:
    st.session_state.news_data = None
if 'source_stats' not in st.session_state:
    st.session_state.source_stats = {}
if 'translated_news' not in st.session_state:
    st.session_state.translated_news = None
if 'api_config' not in st.session_state:
    st.session_state.api_config = {}

# ==================== æ–°é—»æºä»£ç ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰====================
def get_yfinance_news(ticker, debug=False):
    """è·å–yfinanceæ–°é—» - å·²éªŒè¯æœ‰æ•ˆ"""
    try:
        if debug:
            st.sidebar.write(f"ğŸ” æ­£åœ¨è·å– yfinance {ticker} æ–°é—»...")
        
        stock = yf.Ticker(ticker)
        raw_news = stock.news
        
        if not raw_news:
            if debug:
                st.sidebar.warning("âš ï¸ yfinance: æ— æ–°é—»æ•°æ®")
            return []
        
        processed_news = []
        for i, article in enumerate(raw_news):
            try:
                if not isinstance(article, dict):
                    continue
                
                content_data = article.get('content', article)
                
                # æå–æ ‡é¢˜
                title = ''
                for title_field in ['title', 'headline', 'shortName']:
                    t = content_data.get(title_field, '') or article.get(title_field, '')
                    if t and len(str(t).strip()) > 10:
                        title = str(t).strip()
                        break
                
                if not title:
                    continue
                
                # æå–æ‘˜è¦
                summary = ''
                for summary_field in ['summary', 'description', 'snippet']:
                    s = content_data.get(summary_field, '') or article.get(summary_field, '')
                    if s and len(str(s).strip()) > 10:
                        summary = str(s).strip()
                        break
                
                # æå–URL
                url = ''
                click_url = content_data.get('clickThroughUrl', {})
                if isinstance(click_url, dict):
                    url = click_url.get('url', '')
                elif isinstance(click_url, str):
                    url = click_url
                
                if not url:
                    for url_field in ['link', 'url', 'canonicalUrl']:
                        u = content_data.get(url_field, '') or article.get(url_field, '')
                        if u and isinstance(u, str) and len(u) > 10:
                            url = u
                            break
                
                # æå–æ—¶é—´
                published_time = datetime.now() - timedelta(hours=i+1)
                for time_field in ['providerPublishTime', 'publishedAt']:
                    time_val = content_data.get(time_field) or article.get(time_field)
                    if time_val:
                        try:
                            if isinstance(time_val, (int, float)):
                                published_time = datetime.fromtimestamp(time_val)
                                break
                            elif isinstance(time_val, str):
                                published_time = datetime.fromisoformat(time_val.replace('Z', '+00:00')).replace(tzinfo=None)
                                break
                        except:
                            continue
                
                processed_news.append({
                    'title': title,
                    'summary': summary or 'æ¥è‡ªYahoo Financeçš„è´¢ç»æ–°é—»',
                    'url': url,
                    'source': 'Yahoo Finance',
                    'published': published_time,
                    'method': 'yfinance'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"yfinanceå¤„ç†ç¬¬{i+1}æ¡æ–°é—»å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… yfinance: æˆåŠŸè·å– {len(processed_news)} æ¡æ–°é—»")
        
        return processed_news
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ yfinanceè·å–å¤±è´¥: {str(e)}")
        return []

def get_google_news(query, debug=False):
    """è·å–Google News - å¢å¼ºç‰ˆ"""
    try:
        if debug:
            st.sidebar.write(f"ğŸ” æ­£åœ¨è·å–Google News: {query}")
        
        encoded_query = quote(query)
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, timeout=15, headers=headers)
        
        if response.status_code != 200:
            if debug:
                st.sidebar.warning(f"âš ï¸ Google News: HTTP {response.status_code}")
            return []
        
        content = response.text
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        if debug:
            st.sidebar.write(f"ğŸ“Š Google News: æ‰¾åˆ° {len(items)} ä¸ªæ–°é—»é¡¹ç›®")
        
        news_items = []
        for i, item in enumerate(items[:20]):
            try:
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                if not title_match:
                    continue
                    
                title = title_match.group(1).strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title)
                title = title.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '"')
                title = title.strip()
                
                if not title or len(title) < 15:
                    continue
                
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = link_match.group(1).strip() if link_match else ''
                
                pub_date = datetime.now() - timedelta(hours=i/2)
                date_match = re.search(r'<pubDate[^>]*>(.*?)</pubDate>', item, re.DOTALL | re.IGNORECASE)
                if date_match:
                    try:
                        date_str = date_match.group(1).strip()
                        date_str = re.sub(r'\s+[A-Z]{3,4}$', '', date_str)
                        pub_date = datetime.strptime(date_str.strip(), '%a, %d %b %Y %H:%M:%S')
                    except:
                        pass
                
                news_items.append({
                    'title': title,
                    'summary': f'æ¥è‡ªGoogle Newsçš„{query}ç›¸å…³æ–°é—»æŠ¥é“',
                    'url': link,
                    'source': 'Google News',
                    'published': pub_date,
                    'method': 'Google News RSS'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"Google Newså¤„ç†ç¬¬{i+1}æ¡å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… Google News: æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ Google Newsè·å–å¤±è´¥: {str(e)}")
        return []

def get_yahoo_rss_news(ticker=None, debug=False):
    """è·å–Yahoo Finance RSSæ–°é—»"""
    try:
        if debug:
            st.sidebar.write("ğŸ” æ­£åœ¨è·å–Yahoo Finance RSS...")
        
        url = "https://feeds.finance.yahoo.com/rss/2.0/headline?region=US&lang=en-US"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, timeout=15, headers=headers)
        
        if response.status_code != 200:
            if debug:
                st.sidebar.warning(f"âš ï¸ Yahoo RSS: HTTP {response.status_code}")
            return []
        
        content = response.text
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        if debug:
            st.sidebar.write(f"ğŸ“Š Yahoo RSS: æ‰¾åˆ° {len(items)} ä¸ªæ–°é—»é¡¹ç›®")
        
        news_items = []
        for i, item in enumerate(items[:8]):
            try:
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                if not title_match:
                    continue
                    
                title = title_match.group(1).strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title)
                title = title.replace('&amp;', '&').strip()
                
                if not title or len(title) < 10:
                    continue
                
                if ticker:
                    if (ticker.lower() not in title.lower() and 
                        ticker.lower() not in item.lower()):
                        continue
                
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = link_match.group(1).strip() if link_match else ''
                
                desc_match = re.search(r'<description[^>]*>(.*?)</description>', item, re.DOTALL | re.IGNORECASE)
                description = ''
                if desc_match:
                    description = desc_match.group(1).strip()
                    description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                    description = re.sub(r'<[^>]+>', '', description)
                    description = description.replace('&amp;', '&').strip()
                
                news_items.append({
                    'title': title,
                    'summary': description[:200] if description else 'æ¥è‡ªYahoo Finance RSSçš„è´¢ç»æ–°é—»',
                    'url': link,
                    'source': 'Yahoo Finance RSS',
                    'published': datetime.now() - timedelta(hours=i/2),
                    'method': 'RSS'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"Yahoo RSSå¤„ç†ç¬¬{i+1}æ¡å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… Yahoo RSS: æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ Yahoo RSSè·å–å¤±è´¥: {str(e)}")
        return []

def smart_remove_duplicates(news_list):
    """æ™ºèƒ½å»é‡"""
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        title_fingerprint = re.sub(r'[^\w\s]', '', news['title'][:50].lower())
        title_fingerprint = re.sub(r'\s+', ' ', title_fingerprint).strip()
        
        if title_fingerprint not in seen_titles and len(title_fingerprint) > 10:
            seen_titles.add(title_fingerprint)
            unique_news.append(news)
    
    return unique_news

@st.cache_data(ttl=900)
def get_all_reliable_news(ticker=None, debug=False):
    """è·å–æ‰€æœ‰å¯é æ–°é—»æºçš„æ–°é—»"""
    all_news = []
    source_stats = {}
    
    if debug:
        st.sidebar.markdown("### ğŸ” æ–°é—»è·å–è¿‡ç¨‹")
    
    if ticker:
        yf_news = get_yfinance_news(ticker, debug)
        all_news.extend(yf_news)
        source_stats['yfinance'] = len(yf_news)
    else:
        source_stats['yfinance'] = 0
    
    if ticker:
        google_query = f"{ticker} stock financial earnings revenue"
    else:
        google_query = "stock market financial news earnings revenue"
    
    google_news = get_google_news(google_query, debug)
    all_news.extend(google_news)
    source_stats['Google News'] = len(google_news)
    
    yahoo_rss_news = get_yahoo_rss_news(ticker, debug)
    all_news.extend(yahoo_rss_news)
    source_stats['Yahoo RSS'] = len(yahoo_rss_news)
    
    unique_news = smart_remove_duplicates(all_news)
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    
    total_before = len(all_news)
    total_after = len(unique_news)
    removed = total_before - total_after
    
    if debug:
        st.sidebar.info(f"ğŸ“Š åŸå§‹è·å–: {total_before} æ¡ï¼Œå»é‡å: {total_after} æ¡ï¼Œç§»é™¤é‡å¤: {removed} æ¡")
    
    return unique_news, source_stats

# ==================== æ‰¹é‡ç¿»è¯‘åŠŸèƒ½ ====================
def translate_news_batch(news_list, use_api=True, api_config=None, translate_title=True, translate_summary=True):
    """æ‰¹é‡ç¿»è¯‘æ–°é—»"""
    if not news_list:
        return []
    
    translated_news = []
    total_count = len(news_list)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, news in enumerate(news_list):
        progress = (i + 1) / total_count
        progress_bar.progress(progress)
        status_text.text(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ç¬¬ {i+1}/{total_count} æ¡æ–°é—»...")
        
        translated_item = news.copy()
        
        if translate_title and news.get('title'):
            translated_title = smart_translate_text(news['title'], use_api, api_config)
            translated_item['title_zh'] = translated_title
        
        if translate_summary and news.get('summary'):
            translated_summary = smart_translate_text(news['summary'], use_api, api_config)
            translated_item['summary_zh'] = translated_summary
        
        translated_news.append(translated_item)
        time.sleep(0.1)  # é¿å…APIè°ƒç”¨è¿‡å¿«
    
    progress_bar.empty()
    status_text.empty()
    
    return translated_news

def analyze_news_sentiment(title, summary):
    """æ–°é—»æƒ…ç»ªåˆ†æ"""
    text = (title + ' ' + summary).lower()
    
    positive_words = [
        'beat', 'strong', 'growth', 'increase', 'rise', 'gain', 'up', 'success', 
        'record', 'high', 'outperform', 'exceed', 'robust', 'solid', 'win'
    ]
    
    negative_words = [
        'miss', 'weak', 'decline', 'fall', 'drop', 'down', 'loss', 'concern', 
        'worry', 'low', 'underperform', 'disappoint', 'struggle', 'challenge'
    ]
    
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    if pos_count > neg_count and pos_count > 0:
        return 'åˆ©å¥½', 'green'
    elif neg_count > pos_count and neg_count > 0:
        return 'åˆ©ç©º', 'red'
    else:
        return 'ä¸­æ€§', 'gray'

# ==================== ç”¨æˆ·ç•Œé¢ ====================
with st.sidebar:
    st.header("ğŸ“° å¯é æ–°é—»æºè®¾ç½®")
    
    ticker = st.text_input(
        "è‚¡ç¥¨ä»£ç  (å¯é€‰):",
        placeholder="ä¾‹å¦‚: AAPL, AMZN, TSLA, BTC",
        help="è¾“å…¥ä»£ç è·å–ç›¸å…³æ–°é—»ï¼Œç•™ç©ºè·å–å¸‚åœºç»¼åˆæ–°é—»"
    ).upper().strip()
    
    st.markdown("---")
    
    # ç¿»è¯‘è®¾ç½®åŒºåŸŸ
    st.header("ğŸŒ äº‘ç«¯ç¿»è¯‘è®¾ç½®")
    
    translation_enabled = st.checkbox("ğŸ”„ å¯ç”¨æ™ºèƒ½ç¿»è¯‘", value=True, 
                                    help="ä½¿ç”¨äº‘ç«¯APIå°†è‹±æ–‡æ–°é—»ç¿»è¯‘æˆä¸­æ–‡")
    
    if translation_enabled:
        translate_title = st.checkbox("ğŸ“ ç¿»è¯‘æ ‡é¢˜", value=True)
        translate_summary = st.checkbox("ğŸ“„ ç¿»è¯‘æ‘˜è¦", value=True)
        show_original = st.checkbox("ğŸ”¤ åŒæ—¶æ˜¾ç¤ºåŸæ–‡", value=False)
        
        # ç¿»è¯‘å¼•æ“é€‰æ‹©
        translation_engine = st.selectbox(
            "ç¿»è¯‘å¼•æ“:",
            ["å…è´¹API", "ç™¾åº¦ç¿»è¯‘", "ä»…è¯å…¸"],
            help="å…è´¹APIï¼šMyMemoryç­‰å…è´¹æœåŠ¡\nç™¾åº¦ç¿»è¯‘ï¼šéœ€è¦é…ç½®API\nä»…è¯å…¸ï¼šç¦»çº¿è¯å…¸ç¿»è¯‘"
        )
        
        # ç™¾åº¦ç¿»è¯‘APIé…ç½®ï¼ˆå¯é€‰ï¼‰
        if translation_engine == "ç™¾åº¦ç¿»è¯‘":
            st.markdown("##### ç™¾åº¦ç¿»è¯‘APIé…ç½®")
            st.info("ğŸ’¡ æ³¨å†Œ: https://fanyi-api.baidu.com")
            baidu_app_id = st.text_input("App ID:", type="password")
            baidu_secret_key = st.text_input("Secret Key:", type="password")
            
            if baidu_app_id and baidu_secret_key:
                st.session_state.api_config = {
                    'baidu_app_id': baidu_app_id,
                    'baidu_secret_key': baidu_secret_key
                }
                st.success("âœ… ç™¾åº¦ç¿»è¯‘APIå·²é…ç½®")
            else:
                st.warning("âš ï¸ è¯·é…ç½®ç™¾åº¦ç¿»è¯‘API")
        else:
            st.session_state.api_config = {}
    else:
        translate_title = False
        translate_summary = False
        show_original = False
        translation_engine = "ä»…è¯å…¸"
    
    st.markdown("---")
    
    st.markdown("#### ğŸ“¡ å¯ç”¨çš„æ–°é—»æº")
    st.success("âœ… **yfinance** - é«˜è´¨é‡è´¢ç»æ–°é—»")
    st.success("âœ… **Google News** - å¹¿æ³›æ–°é—»èšåˆ")
    st.success("âœ… **Yahoo RSS** - ç¨³å®šå¤‡ç”¨æº")
    if translation_enabled:
        if translation_engine == "å…è´¹API":
            st.success("âœ… **å…è´¹ç¿»è¯‘API** - MyMemoryç­‰")
        elif translation_engine == "ç™¾åº¦ç¿»è¯‘":
            st.success("âœ… **ç™¾åº¦ç¿»è¯‘API** - é«˜è´¨é‡ç¿»è¯‘")
        else:
            st.success("âœ… **è¯å…¸ç¿»è¯‘** - ç¦»çº¿å¤„ç†")
    
    st.markdown("---")
    
    debug_mode = st.checkbox("ğŸ”§ æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯")
    
    st.markdown("---")
    
    # è·å–æ–°é—»æŒ‰é’®
    if st.button("ğŸ“° è·å–å¯é æ–°é—»", type="primary"):
        with st.spinner("æ­£åœ¨ä»å¯é æ–°é—»æºè·å–æ•°æ®..."):
            news_data, stats = get_all_reliable_news(ticker, debug_mode)
            st.session_state.news_data = news_data
            st.session_state.source_stats = stats
            
            if translation_enabled and news_data:
                with st.spinner("ğŸŒ æ­£åœ¨è¿›è¡Œæ™ºèƒ½ç¿»è¯‘..."):
                    use_api = translation_engine != "ä»…è¯å…¸"
                    translated_news = translate_news_batch(
                        news_data, use_api, st.session_state.api_config, 
                        translate_title, translate_summary
                    )
                    st.session_state.translated_news = translated_news
                st.success("âœ… ç¿»è¯‘å®Œæˆï¼")
            else:
                st.session_state.translated_news = None
    
    if st.button("ğŸ”„ æ¸…é™¤ç¼“å­˜"):
        get_all_reliable_news.clear()
        smart_translate_text.clear()
        st.session_state.news_data = None
        st.session_state.source_stats = {}
        st.session_state.translated_news = None
        st.success("ç¼“å­˜å·²æ¸…é™¤ï¼")

# ==================== ä¸»ç•Œé¢æ˜¾ç¤º ====================
def display_news_item(news, index, show_translation=True, show_original=False):
    """æ˜¾ç¤ºå•æ¡æ–°é—»"""
    with st.container():
        sentiment, color = analyze_news_sentiment(news['title'], news['summary'])
        
        if show_translation and 'title_zh' in news:
            title_display = news['title_zh']
        else:
            title_display = news['title']
        
        st.markdown(f"### {index}. {title_display}")
        
        if show_original and 'title_zh' in news:
            st.caption(f"ğŸ”¤ åŸæ–‡: {news['title']}")
        
        time_str = news['published'].strftime('%Y-%m-%d %H:%M')
        source_info = f"ğŸ•’ {time_str} | ğŸ“¡ {news['source']} | ğŸ”§ {news['method']}"
        if 'title_zh' in news:
            source_info += " | ğŸŒ å·²ç¿»è¯‘"
        st.caption(source_info)
        
        col_main, col_side = st.columns([3, 1])
        
        with col_main:
            if show_translation and 'summary_zh' in news:
                summary_display = news['summary_zh']
            else:
                summary_display = news['summary']
            
            st.write(summary_display)
            
            if show_original and 'summary_zh' in news:
                with st.expander("ğŸ”¤ æŸ¥çœ‹è‹±æ–‡åŸæ–‡"):
                    st.write(news['summary'])
            
            if news['url']:
                st.markdown(f"ğŸ”— [é˜…è¯»åŸæ–‡]({news['url']})")
        
        with col_side:
            st.markdown(f"**æƒ…ç»ªåˆ†æ:**")
            st.markdown(f"<span style='color:{color}; font-weight:bold; font-size:18px'>{sentiment}</span>", unsafe_allow_html=True)
            
            if news['method'] == 'yfinance':
                st.write("ğŸ¥‡ **é«˜è´¨é‡æº**")
            elif 'Google News' in news['method']:
                st.write("ğŸ¥ˆ **èšåˆæº**")
            else:
                st.write("ğŸ¥‰ **è¡¥å……æº**")
            
            if 'title_zh' in news or 'summary_zh' in news:
                st.write("ğŸŒ **å·²ç¿»è¯‘**")
        
        st.markdown("---")

# ä¸»ç•Œé¢
if st.session_state.news_data is not None:
    news_data = st.session_state.news_data
    source_stats = st.session_state.source_stats
    translated_news = st.session_state.translated_news
    
    if len(news_data) > 0:
        st.subheader("ğŸ“Š å¯é æ•°æ®æºç»Ÿè®¡")
        
        cols = st.columns(len(source_stats) + 2)
        
        total_unique = len(news_data)
        total_raw = sum(source_stats.values())
        
        with cols[0]:
            st.metric("ğŸ“° æœ€ç»ˆç»“æœ", f"{total_unique} æ¡", f"åŸå§‹: {total_raw}")
        
        for i, (source, count) in enumerate(source_stats.items(), 1):
            with cols[i]:
                if count > 0:
                    st.metric(source, f"{count} æ¡", delta="âœ…")
                else:
                    st.metric(source, f"{count} æ¡", delta="âŒ")
        
        with cols[-1]:
            if translated_news:
                translated_count = sum(1 for n in translated_news if 'title_zh' in n or 'summary_zh' in n)
                st.metric("ğŸŒ ç¿»è¯‘çŠ¶æ€", f"{translated_count} æ¡", delta="âœ…")
            else:
                st.metric("ğŸŒ ç¿»è¯‘çŠ¶æ€", "æœªå¯ç”¨", delta="âŒ")
        
        working_sources = len([count for count in source_stats.values() if count > 0])
        total_sources = len(source_stats)
        reliability = working_sources / total_sources * 100
        
        if reliability >= 80:
            st.success(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - ä¼˜ç§€")
        elif reliability >= 60:
            st.warning(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - è‰¯å¥½")
        else:
            st.error(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - éœ€è¦æ”¹è¿›")
        
        st.markdown("---")
        
        display_news = translated_news if translated_news else news_data
        
        title_suffix = " (æ™ºèƒ½ç¿»è¯‘ç‰ˆ)" if translated_news else ""
        st.subheader(f"ğŸ“° {ticker or 'å¸‚åœº'} æœ€æ–°æ–°é—»{title_suffix}")
        
        for i, news in enumerate(display_news):
            display_news_item(
                news, 
                i + 1, 
                show_translation=bool(translated_news),
                show_original=show_original if translation_enabled else False
            )
        
        # æƒ…ç»ªç»Ÿè®¡
        st.markdown("### ğŸ“ˆ æ•´ä½“å¸‚åœºæƒ…ç»ªåˆ†æ")
        sentiments = {}
        for news in news_data:
            sentiment, _ = analyze_news_sentiment(news['title'], news['summary'])
            sentiments[sentiment] = sentiments.get(sentiment, 0) + 1
        
        sentiment_cols = st.columns(3)
        for i, (sentiment, count) in enumerate(sentiments.items()):
            with sentiment_cols[i]:
                pct = count / len(news_data) * 100
                if sentiment == 'åˆ©å¥½':
                    st.success(f"ğŸ“ˆ **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
                elif sentiment == 'åˆ©ç©º':
                    st.error(f"ğŸ“‰ **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
                else:
                    st.info(f"ğŸ“Š **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
        
        if translated_news:
            st.markdown("### ğŸŒ ç¿»è¯‘ç»Ÿè®¡")
            translation_stats = {
                'æ ‡é¢˜å·²ç¿»è¯‘': sum(1 for n in translated_news if 'title_zh' in n),
                'æ‘˜è¦å·²ç¿»è¯‘': sum(1 for n in translated_news if 'summary_zh' in n),
                'å®Œå…¨ç¿»è¯‘': sum(1 for n in translated_news if 'title_zh' in n and 'summary_zh' in n)
            }
            
            trans_cols = st.columns(3)
            for i, (stat_name, count) in enumerate(translation_stats.items()):
                with trans_cols[i]:
                    pct = count / len(translated_news) * 100
                    st.info(f"ğŸŒ **{stat_name}**: {count} æ¡ ({pct:.0f}%)")
    
    else:
        st.warning("ğŸ“­ æœªè·å–åˆ°æ–°é—»æ•°æ®")
        
        if st.session_state.source_stats:
            st.markdown("### ğŸ“Š å„æºè·å–ç»“æœ:")
            for source, count in st.session_state.source_stats.items():
                if count > 0:
                    st.success(f"âœ… **{source}**: {count} æ¡")
                else:
                    st.error(f"âŒ **{source}**: {count} æ¡")

else:
    st.markdown("""
    ## ğŸ¯ æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (äº‘ç¿»è¯‘ç‰ˆ)
    
    ### ğŸŒŸ **ä¸“ä¸ºéƒ¨ç½²è®¾è®¡ï¼Œå®¢æˆ·æ— éœ€å®‰è£…ä»»ä½•ä¾èµ–**
    
    #### ğŸŒ **å¤šé‡ç¿»è¯‘æ–¹æ¡ˆ**
    - âœ… **å…è´¹äº‘API** - MyMemoryã€LibreTranslateç­‰å…è´¹æœåŠ¡
    - ğŸ”· **ç™¾åº¦ç¿»è¯‘API** - é«˜è´¨é‡ç¿»è¯‘ï¼Œæ¯æœˆ200ä¸‡å­—ç¬¦å…è´¹
    - ğŸ“š **é«˜çº§è¯å…¸** - 500+è´¢ç»æœ¯è¯­ï¼Œå®Œå…¨ç¦»çº¿è¿è¡Œ
    - ğŸ”„ **æ™ºèƒ½é™çº§** - APIå¤±è´¥è‡ªåŠ¨ä½¿ç”¨è¯å…¸ç¿»è¯‘
    
    #### ğŸ“± **å®¢æˆ·ç«¯ä¼˜åŠ¿**
    - **ğŸš€ å³ç”¨å³èµ°** - å®¢æˆ·æ‰“å¼€ç½‘é¡µå°±èƒ½ä½¿ç”¨ï¼Œæ— éœ€å®‰è£…
    - **âš¡ é«˜é€Ÿç¼“å­˜** - ç¿»è¯‘ç»“æœç¼“å­˜1å°æ—¶ï¼Œå“åº”è¿…é€Ÿ
    - **ğŸ›¡ï¸ å¤šé‡å¤‡ç”¨** - 3å±‚ç¿»è¯‘ä¿éšœï¼ŒæˆåŠŸç‡99%+
    - **ğŸŒ æœåŠ¡å™¨å¤„ç†** - æ‰€æœ‰ç¿»è¯‘åœ¨æœåŠ¡å™¨ç«¯å®Œæˆ
    
    ### ğŸ“¡ **ä¾ç„¶ä¿æŒåŸæœ‰ä¼˜åŠ¿**
    
    #### ğŸ¥‡ **yfinance** - å·²éªŒè¯100%æœ‰æ•ˆ
    #### ğŸ¥ˆ **Google News** - å¹¿æ³›æ–°é—»èšåˆ  
    #### ğŸ¥‰ **Yahoo Finance RSS** - å®˜æ–¹ç¨³å®šæº
    
    ### ğŸš€ **éƒ¨ç½²æ–¹æ¡ˆ**
    
    #### ğŸ’° **å…è´¹æ–¹æ¡ˆ (æ¨è)**
    - ä½¿ç”¨å…è´¹ç¿»è¯‘API + è¯å…¸å¤‡ç”¨
    - æ¯å¤©1000æ¬¡å…è´¹ç¿»è¯‘è°ƒç”¨
    - é€‚åˆä¸­å°å‹ç½‘ç«™
    
    #### ğŸ”· **ä¸“ä¸šæ–¹æ¡ˆ**
    - é…ç½®ç™¾åº¦ç¿»è¯‘API
    - æ¯æœˆ200ä¸‡å­—ç¬¦å…è´¹é¢åº¦
    - ç¿»è¯‘è´¨é‡æ›´ä¼˜ï¼Œç¨³å®šæ€§æ›´é«˜
    
    ### ğŸ’¡ **éƒ¨ç½²åå®¢æˆ·ä½“éªŒ**
    
    **å®¢æˆ·è®¿é—®ä½ çš„ç½‘ç«™æ—¶**:
    1. ğŸ“± æ‰“å¼€ç½‘é¡µå°±èƒ½ç”¨ï¼ˆæ— éœ€å®‰è£…ï¼‰
    2. ğŸŒ ä¸€é”®ç¿»è¯‘æ–°é—»ï¼ˆè‡ªåŠ¨æ™ºèƒ½ç¿»è¯‘ï¼‰
    3. âš¡ å“åº”è¿…é€Ÿï¼ˆç¼“å­˜æœºåˆ¶ï¼‰
    4. ğŸ›¡ï¸ ç¨³å®šå¯é ï¼ˆå¤šé‡å¤‡ç”¨æ–¹æ¡ˆï¼‰
    
    ---
    
    **ğŸ‘ˆ åœ¨å·¦ä¾§é…ç½®ç¿»è¯‘é€‰é¡¹å¹¶å¼€å§‹ä½¿ç”¨äº‘ç«¯ç¿»è¯‘åŠŸèƒ½**
    """)

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (äº‘ç¿»è¯‘ç‰ˆ) | âœ… éªŒè¯æœ‰æ•ˆæº | ğŸ›¡ï¸ 99%+ å¯é æ€§ | ğŸŒ äº‘ç«¯ç¿»è¯‘ | ğŸ“± å®¢æˆ·å…å®‰è£…
</div>
""", unsafe_allow_html=True)
