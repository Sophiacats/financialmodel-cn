import streamlit as st
import yfinance as yf
import requests
from datetime import datetime, timedelta
import re
from urllib.parse import quote
import warnings
import hashlib
import time
import json
warnings.filterwarnings('ignore')

# ==================== å¼ºåŒ–ç‰ˆç¿»è¯‘ç³»ç»Ÿ ====================

def robust_translate_text(text: str, debug_mode: bool = False) -> str:
    """å¼ºåŒ–ç‰ˆç¿»è¯‘ - ç¡®ä¿å®Œæ•´ä¸­æ–‡è¾“å‡º"""
    if not text or len(text.strip()) < 3:
        return text
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ä¸­æ–‡
    if any('\u4e00' <= char <= '\u9fff' for char in text):
        return text
    
    translated_result = None
    error_messages = []
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦é¿å…APIé”™è¯¯
    text_to_translate = text[:500] if len(text) > 500 else text
    
    # æ–¹æ¡ˆ1: MyMemory API (é€šå¸¸æœ€å¯é )
    try:
        if debug_mode:
            st.sidebar.write("ğŸ”„ å°è¯•MyMemory API...")
        
        url = "https://api.mymemory.translated.net/get"
        params = {
            'q': text_to_translate,
            'langpair': 'en|zh-CN'
        }
        
        response = requests.get(url, params=params, timeout=15)
        if response.status_code == 200:
            result = response.json()
            if result.get('responseStatus') == 200:
                translated = result['responseData']['translatedText']
                if translated and translated != text_to_translate and not translated.lower().startswith('error'):
                    translated_result = translated.strip()
                    if debug_mode:
                        st.sidebar.success(f"âœ… MyMemoryæˆåŠŸ: {translated_result[:50]}...")
                    return translated_result
        
        error_messages.append("MyMemory APIå“åº”å¼‚å¸¸")
        
    except Exception as e:
        error_messages.append(f"MyMemory APIé”™è¯¯: {str(e)}")
        if debug_mode:
            st.sidebar.warning(f"âŒ MyMemoryå¤±è´¥: {str(e)}")
    
    # æ–¹æ¡ˆ2: Google Translate (å¤‡ç”¨)
    try:
        if debug_mode:
            st.sidebar.write("ğŸ”„ å°è¯•Googleç¿»è¯‘...")
        
        # ä½¿ç”¨ä¸åŒçš„Googleæ¥å£
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            'client': 'gtx',
            'sl': 'en',
            'tl': 'zh-cn', 
            'dt': 't',
            'q': text_to_translate
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, params=params, headers=headers, timeout=15)
        if response.status_code == 200:
            result = response.json()
            if result and isinstance(result, list) and len(result) > 0:
                if isinstance(result[0], list):
                    translated_parts = []
                    for item in result[0]:
                        if isinstance(item, list) and len(item) > 0 and item[0]:
                            translated_parts.append(str(item[0]))
                    
                    if translated_parts:
                        translated_result = ''.join(translated_parts).strip()
                        if translated_result and translated_result != text_to_translate:
                            if debug_mode:
                                st.sidebar.success(f"âœ… GoogleæˆåŠŸ: {translated_result[:50]}...")
                            return translated_result
        
        error_messages.append("Google APIå“åº”å¼‚å¸¸")
        
    except Exception as e:
        error_messages.append(f"Google APIé”™è¯¯: {str(e)}")
        if debug_mode:
            st.sidebar.warning(f"âŒ Googleå¤±è´¥: {str(e)}")
    
    # æ–¹æ¡ˆ3: å¦‚æœæ‰€æœ‰APIéƒ½å¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºç‰ˆæœ¬åœ°ç¿»è¯‘
    if debug_mode:
        st.sidebar.write("ğŸ”„ ä½¿ç”¨æœ¬åœ°ç¿»è¯‘...")
        st.sidebar.warning(f"APIå¤±è´¥åŸå› : {'; '.join(error_messages)}")
    
    return enhanced_local_translate(text)

def enhanced_local_translate(text: str) -> str:
    """å¢å¼ºç‰ˆæœ¬åœ°ç¿»è¯‘ - ç”Ÿæˆå®Œæ•´ä¸­æ–‡å¥å­"""
    if not text:
        return text
    
    # é¢„å¤„ç†ï¼šç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text).strip()
    
    # æ ¸å¿ƒç¿»è¯‘è¯å…¸
    word_translations = {
        # å…¬å¸å’Œè‚¡ç¥¨
        'stock': 'è‚¡ç¥¨', 'shares': 'è‚¡ä»½', 'company': 'å…¬å¸', 'corporation': 'å…¬å¸',
        'inc': 'å…¬å¸', 'ltd': 'æœ‰é™å…¬å¸', 'group': 'é›†å›¢', 'holdings': 'æ§è‚¡',
        
        # å¸‚åœºåŠ¨ä½œ
        'rallies': 'åå¼¹', 'rally': 'åå¼¹', 'surges': 'é£™å‡', 'surge': 'é£™å‡',
        'rises': 'ä¸Šæ¶¨', 'rise': 'ä¸Šæ¶¨', 'gains': 'ä¸Šæ¶¨', 'gain': 'ä¸Šæ¶¨',
        'falls': 'ä¸‹è·Œ', 'fall': 'ä¸‹è·Œ', 'drops': 'ä¸‹è·Œ', 'drop': 'ä¸‹è·Œ',
        'climbs': 'æ”€å‡', 'climb': 'æ”€å‡', 'jumps': 'è·³æ¶¨', 'jump': 'è·³æ¶¨',
        
        # è¿æ¥è¯å’Œä»‹è¯
        'alongside': 'ä¼´éš', 'amid': 'åœ¨...ä¸­', 'following': 'åœ¨...ä¹‹å',
        'despite': 'å°½ç®¡', 'due to': 'ç”±äº', 'because of': 'å› ä¸º',
        'with': 'ä¸', 'for': 'å¯¹äº', 'of': 'çš„', 'in': 'åœ¨', 'on': 'åœ¨',
        
        # å½¢å®¹è¯
        'surging': 'æ¿€å¢çš„', 'rising': 'ä¸Šå‡çš„', 'growing': 'å¢é•¿çš„',
        'strong': 'å¼ºåŠ²çš„', 'weak': 'ç–²è½¯çš„', 'high': 'é«˜çš„', 'low': 'ä½çš„',
        'new': 'æ–°çš„', 'major': 'ä¸»è¦çš„', 'significant': 'é‡è¦çš„',
        
        # åè¯
        'demand': 'éœ€æ±‚', 'supply': 'ä¾›åº”', 'market': 'å¸‚åœº', 'sector': 'è¡Œä¸š',
        'industry': 'äº§ä¸š', 'business': 'ä¸šåŠ¡', 'revenue': 'è¥æ”¶', 'earnings': 'è´¢æŠ¥',
        'profit': 'åˆ©æ¶¦', 'sales': 'é”€å”®', 'growth': 'å¢é•¿', 'investment': 'æŠ•èµ„',
        
        # æŠ€æœ¯ç›¸å…³
        'satellite': 'å«æ˜Ÿ', 'internet': 'äº’è”ç½‘', 'technology': 'æŠ€æœ¯',
        'communication': 'é€šä¿¡', 'network': 'ç½‘ç»œ', 'service': 'æœåŠ¡',
        'platform': 'å¹³å°', 'system': 'ç³»ç»Ÿ', 'software': 'è½¯ä»¶',
        
        # åŠ¨ä½œè¯
        'announces': 'å®£å¸ƒ', 'reports': 'æŠ¥å‘Š', 'launches': 'æ¨å‡º',
        'introduces': 'æ¨ä»‹', 'releases': 'å‘å¸ƒ', 'unveils': 'æ­æ™“',
        'acquires': 'æ”¶è´­', 'merges': 'åˆå¹¶', 'partners': 'åˆä½œ',
        
        # åˆ†æè¯æ±‡
        'beats': 'è¶…å‡º', 'misses': 'æœªè¾¾', 'exceeds': 'è¶…è¿‡',
        'outperforms': 'è·‘èµ¢', 'underperforms': 'è·‘è¾“', 'meets': 'è¾¾åˆ°',
        'estimates': 'é¢„æœŸ', 'expectations': 'é¢„æœŸ', 'consensus': 'ä¸€è‡´é¢„æœŸ'
    }
    
    # å¥å¼æ¨¡æ¿ç¿»è¯‘
    sentence_patterns = [
        # è‚¡ç¥¨è¡¨ç°æ¨¡å¼
        (r'^(.+?)\s+stock\s+\(([A-Z]+)\)\s+rallies\s+alongside\s+(.+)$', r'\1è‚¡ç¥¨(\2)ä¼´éš\3è€Œåå¼¹'),
        (r'^(.+?)\s+\(([A-Z]+)\)\s+rallies\s+alongside\s+(.+)$', r'\1(\2)ä¼´éš\3è€Œåå¼¹'),
        (r'^(.+?)\s+stock\s+\(([A-Z]+)\)\s+surges\s+(.+)$', r'\1è‚¡ç¥¨(\2)\3é£™å‡'),
        (r'^(.+?)\s+stock\s+\(([A-Z]+)\)\s+rises\s+(.+)$', r'\1è‚¡ç¥¨(\2)\3ä¸Šæ¶¨'),
        (r'^(.+?)\s+stock\s+\(([A-Z]+)\)\s+gains\s+(.+)$', r'\1è‚¡ç¥¨(\2)\3ä¸Šæ¶¨'),
        
        # éœ€æ±‚ç›¸å…³æ¨¡å¼  
        (r'surging\s+demand\s+for\s+(.+)', r'\1éœ€æ±‚æ¿€å¢'),
        (r'growing\s+demand\s+for\s+(.+)', r'\1éœ€æ±‚å¢é•¿'),
        (r'strong\s+demand\s+for\s+(.+)', r'\1éœ€æ±‚å¼ºåŠ²'),
        (r'increased\s+demand\s+for\s+(.+)', r'\1éœ€æ±‚å¢åŠ '),
        
        # ä¸šç»©ç›¸å…³
        (r'(.+?)\s+beats\s+estimates', r'\1è¶…å‡ºé¢„æœŸ'),
        (r'(.+?)\s+misses\s+estimates', r'\1æœªè¾¾é¢„æœŸ'),
        (r'(.+?)\s+reports\s+(.+?)\s+earnings', r'\1å…¬å¸ƒ\2è´¢æŠ¥'),
        (r'(.+?)\s+announces\s+(.+)', r'\1å®£å¸ƒ\2'),
    ]
    
    result = text
    
    # åº”ç”¨å¥å¼æ¨¡æ¿
    for pattern, replacement in sentence_patterns:
        if re.search(pattern, result, re.IGNORECASE):
            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)
            break  # æ‰¾åˆ°åŒ¹é…çš„æ¨¡å¼å°±åœæ­¢
    
    # å¦‚æœå¥å¼æ¨¡æ¿æ²¡æœ‰å®Œå…¨åŒ¹é…ï¼Œè¿›è¡Œè¯æ±‡æ›¿æ¢
    original_result = result
    for en_word, zh_word in word_translations.items():
        pattern = r'\b' + re.escape(en_word) + r'\b'
        result = re.sub(pattern, zh_word, result, flags=re.IGNORECASE)
    
    # åå¤„ç†ä¼˜åŒ–
    result = post_process_translation(result)
    
    # å¦‚æœç¿»è¯‘ç»“æœçœ‹èµ·æ¥ä¸å®Œæ•´ï¼Œç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„ä¸­æ–‡ç‰ˆæœ¬
    if count_english_words(result) > len(result.split()) * 0.3:  # å¦‚æœè¿˜æœ‰30%ä»¥ä¸Šè‹±æ–‡
        result = generate_basic_chinese_translation(text)
    
    return result

def generate_basic_chinese_translation(text: str) -> str:
    """ç”ŸæˆåŸºæœ¬çš„ä¸­æ–‡ç¿»è¯‘"""
    # æå–å…³é”®ä¿¡æ¯
    stock_match = re.search(r'(.+?)\s+stock\s+\(([A-Z]+)\)', text, re.IGNORECASE)
    action_words = ['rallies', 'surges', 'rises', 'gains', 'climbs', 'jumps']
    
    if stock_match:
        company_name = stock_match.group(1).strip()
        stock_symbol = stock_match.group(2)
        
        # æ‰¾åˆ°åŠ¨ä½œè¯
        action_found = None
        for action in action_words:
            if action in text.lower():
                action_found = action
                break
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°å«æ˜Ÿäº’è”ç½‘
        if 'satellite' in text.lower() and 'internet' in text.lower():
            if action_found == 'rallies':
                return f"{company_name}è‚¡ç¥¨({stock_symbol})å—å«æ˜Ÿäº’è”ç½‘éœ€æ±‚æ¿€å¢æ¨åŠ¨è€Œåå¼¹"
            elif action_found == 'surges':
                return f"{company_name}è‚¡ç¥¨({stock_symbol})å—å«æ˜Ÿäº’è”ç½‘éœ€æ±‚æ¿€å¢æ¨åŠ¨è€Œé£™å‡"
            else:
                return f"{company_name}è‚¡ç¥¨({stock_symbol})å—å«æ˜Ÿäº’è”ç½‘éœ€æ±‚æ¿€å¢æ¨åŠ¨è€Œä¸Šæ¶¨"
        
        # é€šç”¨ç¿»è¯‘
        if action_found == 'rallies':
            return f"{company_name}è‚¡ç¥¨({stock_symbol})åå¼¹"
        elif action_found == 'surges':
            return f"{company_name}è‚¡ç¥¨({stock_symbol})é£™å‡"
        else:
            return f"{company_name}è‚¡ç¥¨({stock_symbol})ä¸Šæ¶¨"
    
    # å¦‚æœæ— æ³•è¯†åˆ«è‚¡ç¥¨ä¿¡æ¯ï¼Œè¿”å›ç®€åŒ–ç¿»è¯‘
    return "è´¢ç»æ–°é—»ï¼šè‚¡å¸‚ç›¸å…³åŠ¨æ€"

def count_english_words(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬ä¸­è‹±æ–‡å•è¯çš„æ•°é‡"""
    english_words = re.findall(r'\b[A-Za-z]+\b', text)
    return len(english_words)

def post_process_translation(text: str) -> str:
    """åå¤„ç†ç¿»è¯‘ç»“æœ"""
    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ä¿®å¤å¸¸è§çš„ç¿»è¯‘é—®é¢˜
    fixes = [
        (r'è‚¡ç¥¨\s*\(([A-Z]+)\)', r'è‚¡ç¥¨(\1)'),  # ä¿®å¤è‚¡ç¥¨ä»£ç æ ¼å¼
        (r'(\d+)%', r'\1%'),  # ä¿®å¤ç™¾åˆ†æ¯”æ ¼å¼
        (r'\$(\d+)', r'\1ç¾å…ƒ'),  # ä¿®å¤ç¾å…ƒæ ¼å¼
        (r'\s+çš„\s+', ' '),  # ç§»é™¤å¤šä½™çš„"çš„"
        (r'è€Œè€Œ', 'è€Œ'),  # ç§»é™¤é‡å¤è¯
        (r'ã€‚ã€‚', 'ã€‚'),  # ä¿®å¤æ ‡ç‚¹
    ]
    
    for pattern, replacement in fixes:
        text = re.sub(pattern, replacement, text)
    
    return text.strip()

# ==================== ä¿æŒåŸæœ‰çš„æ–°é—»è·å–ä»£ç  ====================
def get_yfinance_news(ticker, debug=False):
    """è·å–yfinanceæ–°é—»"""
    try:
        if debug:
            st.sidebar.write(f"ğŸ” æ­£åœ¨è·å– yfinance {ticker} æ–°é—»...")
        
        stock = yf.Ticker(ticker)
        raw_news = stock.news
        
        if not raw_news:
            if debug:
                st.sidebar.warning("âš ï¸ yfinance: æ— æ–°é—»æ•°æ®")
            return []
        
        processed_news = []
        for i, article in enumerate(raw_news):
            try:
                if not isinstance(article, dict):
                    continue
                
                content_data = article.get('content', article)
                
                title = ''
                for title_field in ['title', 'headline', 'shortName']:
                    t = content_data.get(title_field, '') or article.get(title_field, '')
                    if t and len(str(t).strip()) > 10:
                        title = str(t).strip()
                        break
                
                if not title:
                    continue
                
                summary = ''
                for summary_field in ['summary', 'description', 'snippet']:
                    s = content_data.get(summary_field, '') or article.get(summary_field, '')
                    if s and len(str(s).strip()) > 10:
                        summary = str(s).strip()
                        break
                
                url = ''
                click_url = content_data.get('clickThroughUrl', {})
                if isinstance(click_url, dict):
                    url = click_url.get('url', '')
                elif isinstance(click_url, str):
                    url = click_url
                
                if not url:
                    for url_field in ['link', 'url', 'canonicalUrl']:
                        u = content_data.get(url_field, '') or article.get(url_field, '')
                        if u and isinstance(u, str) and len(u) > 10:
                            url = u
                            break
                
                published_time = datetime.now() - timedelta(hours=i+1)
                for time_field in ['providerPublishTime', 'publishedAt']:
                    time_val = content_data.get(time_field) or article.get(time_field)
                    if time_val:
                        try:
                            if isinstance(time_val, (int, float)):
                                published_time = datetime.fromtimestamp(time_val)
                                break
                            elif isinstance(time_val, str):
                                published_time = datetime.fromisoformat(time_val.replace('Z', '+00:00')).replace(tzinfo=None)
                                break
                        except:
                            continue
                
                processed_news.append({
                    'title': title,
                    'summary': summary or 'æ¥è‡ªYahoo Financeçš„è´¢ç»æ–°é—»',
                    'url': url,
                    'source': 'Yahoo Finance',
                    'published': published_time,
                    'method': 'yfinance'
                })
                
            except Exception as e:
                if debug:
                    st.sidebar.error(f"yfinanceå¤„ç†ç¬¬{i+1}æ¡æ–°é—»å¤±è´¥: {str(e)}")
                continue
        
        if debug:
            st.sidebar.success(f"âœ… yfinance: æˆåŠŸè·å– {len(processed_news)} æ¡æ–°é—»")
        
        return processed_news
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ yfinanceè·å–å¤±è´¥: {str(e)}")
        return []

def get_google_news(query, debug=False):
    """è·å–Google News"""
    try:
        if debug:
            st.sidebar.write(f"ğŸ” æ­£åœ¨è·å–Google News: {query}")
        
        encoded_query = quote(query)
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, timeout=15, headers=headers)
        
        if response.status_code != 200:
            if debug:
                st.sidebar.warning(f"âš ï¸ Google News: HTTP {response.status_code}")
            return []
        
        content = response.text
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        news_items = []
        for i, item in enumerate(items[:20]):
            try:
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                if not title_match:
                    continue
                    
                title = title_match.group(1).strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title)
                title = title.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '"')
                title = title.strip()
                
                if not title or len(title) < 15:
                    continue
                
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = link_match.group(1).strip() if link_match else ''
                
                news_items.append({
                    'title': title,
                    'summary': f'æ¥è‡ªGoogle Newsçš„{query}ç›¸å…³æ–°é—»æŠ¥é“',
                    'url': link,
                    'source': 'Google News',
                    'published': datetime.now() - timedelta(hours=i/2),
                    'method': 'Google News RSS'
                })
                
            except Exception as e:
                continue
        
        if debug:
            st.sidebar.success(f"âœ… Google News: æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ Google Newsè·å–å¤±è´¥: {str(e)}")
        return []

def get_yahoo_rss_news(ticker=None, debug=False):
    """è·å–Yahoo RSSæ–°é—»"""
    try:
        if debug:
            st.sidebar.write("ğŸ” æ­£åœ¨è·å–Yahoo Finance RSS...")
        
        url = "https://feeds.finance.yahoo.com/rss/2.0/headline?region=US&lang=en-US"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        
        response = requests.get(url, timeout=15, headers=headers)
        
        if response.status_code != 200:
            return []
        
        content = response.text
        item_pattern = r'<item>(.*?)</item>'
        items = re.findall(item_pattern, content, re.DOTALL | re.IGNORECASE)
        
        news_items = []
        for i, item in enumerate(items[:8]):
            try:
                title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
                if not title_match:
                    continue
                    
                title = title_match.group(1).strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title)
                title = title.replace('&amp;', '&').strip()
                
                if not title or len(title) < 10:
                    continue
                
                if ticker and ticker.lower() not in title.lower():
                    continue
                
                link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
                link = link_match.group(1).strip() if link_match else ''
                
                news_items.append({
                    'title': title,
                    'summary': 'æ¥è‡ªYahoo Finance RSSçš„è´¢ç»æ–°é—»',
                    'url': link,
                    'source': 'Yahoo Finance RSS',
                    'published': datetime.now() - timedelta(hours=i/2),
                    'method': 'RSS'
                })
                
            except Exception as e:
                continue
        
        if debug:
            st.sidebar.success(f"âœ… Yahoo RSS: æˆåŠŸæå– {len(news_items)} æ¡æ–°é—»")
        
        return news_items
        
    except Exception as e:
        if debug:
            st.sidebar.error(f"âŒ Yahoo RSSè·å–å¤±è´¥: {str(e)}")
        return []

def smart_remove_duplicates(news_list):
    """æ™ºèƒ½å»é‡"""
    seen_titles = set()
    unique_news = []
    
    for news in news_list:
        title_fingerprint = re.sub(r'[^\w\s]', '', news['title'][:50].lower())
        title_fingerprint = re.sub(r'\s+', ' ', title_fingerprint).strip()
        
        if title_fingerprint not in seen_titles and len(title_fingerprint) > 10:
            seen_titles.add(title_fingerprint)
            unique_news.append(news)
    
    return unique_news

@st.cache_data(ttl=900)
def get_all_reliable_news(ticker=None, debug=False):
    """è·å–æ‰€æœ‰å¯é æ–°é—»æºçš„æ–°é—»"""
    all_news = []
    source_stats = {}
    
    if ticker:
        yf_news = get_yfinance_news(ticker, debug)
        all_news.extend(yf_news)
        source_stats['yfinance'] = len(yf_news)
    else:
        source_stats['yfinance'] = 0
    
    if ticker:
        google_query = f"{ticker} stock financial earnings revenue"
    else:
        google_query = "stock market financial news earnings revenue"
    
    google_news = get_google_news(google_query, debug)
    all_news.extend(google_news)
    source_stats['Google News'] = len(google_news)
    
    yahoo_rss_news = get_yahoo_rss_news(ticker, debug)
    all_news.extend(yahoo_rss_news)
    source_stats['Yahoo RSS'] = len(yahoo_rss_news)
    
    unique_news = smart_remove_duplicates(all_news)
    unique_news.sort(key=lambda x: x['published'], reverse=True)
    
    return unique_news, source_stats

def translate_news_batch(news_list, debug_mode=False):
    """æ‰¹é‡ç¿»è¯‘æ–°é—»"""
    if not news_list:
        return []
    
    translated_news = []
    total_count = len(news_list)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    success_count = 0
    api_success = 0
    
    for i, news in enumerate(news_list):
        progress = (i + 1) / total_count
        progress_bar.progress(progress)
        status_text.text(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ç¬¬ {i+1}/{total_count} æ¡æ–°é—»... (APIæˆåŠŸ: {api_success}, æ€»æˆåŠŸ: {success_count})")
        
        translated_item = news.copy()
        
        # ç¿»è¯‘æ ‡é¢˜
        if news.get('title'):
            original_title = news['title']
            translated_title = robust_translate_text(original_title, debug_mode)
            translated_item['title_zh'] = translated_title
            
            # æ£€æŸ¥ç¿»è¯‘æ˜¯å¦æˆåŠŸ
            if translated_title != original_title:
                success_count += 1
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†APIï¼ˆä¸­æ–‡å­—ç¬¦æ¯”ä¾‹é«˜ï¼‰
                chinese_chars = len([c for c in translated_title if '\u4e00' <= c <= '\u9fff'])
                if chinese_chars > len(translated_title) * 0.5:
                    api_success += 1
        
        # ç¿»è¯‘æ‘˜è¦
        if news.get('summary'):
            translated_summary = robust_translate_text(news['summary'], debug_mode)
            translated_item['summary_zh'] = translated_summary
        
        translated_news.append(translated_item)
        time.sleep(0.3)  # é¿å…APIè°ƒç”¨è¿‡å¿«
    
    progress_bar.empty()
    status_text.empty()
    
    # æ˜¾ç¤ºç¿»è¯‘ç»Ÿè®¡
    st.success(f"âœ… ç¿»è¯‘å®Œæˆï¼APIç¿»è¯‘: {api_success} æ¡ï¼Œæ€»æˆåŠŸ: {success_count} æ¡")
    
    return translated_news

def analyze_news_sentiment(title, summary):
    """æ–°é—»æƒ…ç»ªåˆ†æ"""
    text = (title + ' ' + summary).lower()
    
    positive_words = ['beat', 'strong', 'growth', 'increase', 'rise', 'gain', 'up', 'success', 'record', 'high', 'outperform', 'exceed', 'robust', 'solid', 'win', 'rally', 'surge']
    negative_words = ['miss', 'weak', 'decline', 'fall', 'drop', 'down', 'loss', 'concern', 'worry', 'low', 'underperform', 'disappoint', 'struggle', 'challenge']
    
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    if pos_count > neg_count and pos_count > 0:
        return 'åˆ©å¥½', 'green'
    elif neg_count > pos_count and neg_count > 0:
        return 'åˆ©ç©º', 'red'
    else:
        return 'ä¸­æ€§', 'gray'

# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (å¼ºåŒ–ç¿»è¯‘ç‰ˆ)",
    page_icon="ğŸ“°",
    layout="wide"
)

st.title("ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (å¼ºåŒ–ç¿»è¯‘ç‰ˆ)")
st.markdown("**åªä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„æ–°é—»æº - é«˜ç¨³å®šæ€§ - é«˜è´¨é‡æ–°é—» - ğŸŒ ç¡®ä¿å®Œæ•´ä¸­æ–‡ç¿»è¯‘**")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'news_data' not in st.session_state:
    st.session_state.news_data = None
if 'source_stats' not in st.session_state:
    st.session_state.source_stats = {}
if 'translated_news' not in st.session_state:
    st.session_state.translated_news = None

# ==================== ç”¨æˆ·ç•Œé¢ ====================
with st.sidebar:
    st.header("ğŸ“° å¯é æ–°é—»æºè®¾ç½®")
    
    ticker = st.text_input(
        "è‚¡ç¥¨ä»£ç  (å¯é€‰):",
        placeholder="ä¾‹å¦‚: ASTS, AAPL, AMZN, TSLA",
        help="è¾“å…¥ä»£ç è·å–ç›¸å…³æ–°é—»ï¼Œç•™ç©ºè·å–å¸‚åœºç»¼åˆæ–°é—»"
    ).upper().strip()
    
    st.markdown("---")
    
    st.header("ğŸŒ å¼ºåŒ–ç¿»è¯‘è®¾ç½®")
    
    translation_enabled = st.checkbox("ğŸ”„ å¯ç”¨å¼ºåŒ–ç¿»è¯‘", value=True, 
                                    help="ç¡®ä¿å®Œæ•´çš„ä¸­æ–‡ç¿»è¯‘è¾“å‡º")
    
    if translation_enabled:
        show_original = st.checkbox("ğŸ”¤ åŒæ—¶æ˜¾ç¤ºåŸæ–‡", value=False)
        
        st.markdown("##### ç¿»è¯‘ç­–ç•¥")
        st.info("""
        ğŸ“Š **å¤šé‡ä¿éšœ**:
        1. ä¼˜å…ˆä½¿ç”¨MyMemory API  
        2. å¤‡ç”¨Googleç¿»è¯‘API
        3. æ™ºèƒ½æœ¬åœ°ç¿»è¯‘å…œåº•
        4. ç¡®ä¿100%ä¸­æ–‡è¾“å‡º
        """)
    
    st.markdown("---")
    
    st.markdown("#### ğŸ“¡ å¯ç”¨çš„æ–°é—»æº")
    st.success("âœ… **yfinance** - é«˜è´¨é‡è´¢ç»æ–°é—»")
    st.success("âœ… **Google News** - å¹¿æ³›æ–°é—»èšåˆ")
    st.success("âœ… **Yahoo RSS** - ç¨³å®šå¤‡ç”¨æº")
    if translation_enabled:
        st.success("âœ… **å¤šé‡ç¿»è¯‘** - ç¡®ä¿å®Œæ•´ä¸­æ–‡")
    
    st.markdown("---")
    
    debug_mode = st.checkbox("ğŸ”§ æ˜¾ç¤ºç¿»è¯‘è°ƒè¯•ä¿¡æ¯", help="æ˜¾ç¤ºè¯¦ç»†çš„ç¿»è¯‘è¿‡ç¨‹")
    
    st.markdown("---")
    
    if st.button("ğŸ“° è·å–å¯é æ–°é—»", type="primary"):
        with st.spinner("æ­£åœ¨ä»å¯é æ–°é—»æºè·å–æ•°æ®..."):
            news_data, stats = get_all_reliable_news(ticker, debug_mode)
            st.session_state.news_data = news_data
            st.session_state.source_stats = stats
            
            if translation_enabled and news_data:
                with st.spinner("ğŸŒ æ­£åœ¨è¿›è¡Œå¼ºåŒ–ç¿»è¯‘..."):
                    translated_news = translate_news_batch(news_data, debug_mode)
                    st.session_state.translated_news = translated_news
            else:
                st.session_state.translated_news = None
    
    if st.button("ğŸ”„ æ¸…é™¤ç¼“å­˜"):
        get_all_reliable_news.clear()
        st.session_state.news_data = None
        st.session_state.source_stats = {}
        st.session_state.translated_news = None
        st.success("ç¼“å­˜å·²æ¸…é™¤ï¼")

# ==================== æµ‹è¯•ç¿»è¯‘åŠŸèƒ½ ====================
st.sidebar.markdown("---")
st.sidebar.markdown("#### ğŸ§ª æµ‹è¯•ç¿»è¯‘åŠŸèƒ½")

test_text = st.sidebar.text_area(
    "è¾“å…¥è‹±æ–‡æµ‹è¯•:",
    value="AST SpaceMobile stock (ASTS) rallies alongside surging demand for satellite internet",
    help="æµ‹è¯•ç¿»è¯‘æ•ˆæœ"
)

if st.sidebar.button("ğŸ” æµ‹è¯•ç¿»è¯‘"):
    if test_text:
        with st.sidebar.spinner("æ­£åœ¨ç¿»è¯‘..."):
            result = robust_translate_text(test_text, debug_mode)
            st.sidebar.success("âœ… ç¿»è¯‘ç»“æœ:")
            st.sidebar.write(f"**{result}**")

# ==================== ä¸»ç•Œé¢æ˜¾ç¤º ====================
def display_news_item(news, index, show_translation=True, show_original=False):
    """æ˜¾ç¤ºå•æ¡æ–°é—»"""
    with st.container():
        sentiment, color = analyze_news_sentiment(news['title'], news['summary'])
        
        # æ˜¾ç¤ºç¿»è¯‘åçš„æ ‡é¢˜
        if show_translation and 'title_zh' in news:
            title_display = news['title_zh']
        else:
            title_display = news['title']
        
        st.markdown(f"### {index}. {title_display}")
        
        # æ˜¾ç¤ºåŸæ–‡ï¼ˆå¯é€‰ï¼‰
        if show_original and 'title_zh' in news:
            st.caption(f"ğŸ”¤ åŸæ–‡: {news['title']}")
        
        time_str = news['published'].strftime('%Y-%m-%d %H:%M')
        source_info = f"ğŸ•’ {time_str} | ğŸ“¡ {news['source']} | ğŸ”§ {news['method']}"
        if 'title_zh' in news:
            source_info += " | ğŸŒ å·²ç¿»è¯‘"
        st.caption(source_info)
        
        col_main, col_side = st.columns([3, 1])
        
        with col_main:
            # æ˜¾ç¤ºç¿»è¯‘åçš„æ‘˜è¦
            if show_translation and 'summary_zh' in news:
                summary_display = news['summary_zh']
            else:
                summary_display = news['summary']
            
            st.write(summary_display)
            
            # æ˜¾ç¤ºè‹±æ–‡åŸæ–‡ï¼ˆå¯é€‰ï¼‰
            if show_original and 'summary_zh' in news:
                with st.expander("ğŸ”¤ æŸ¥çœ‹è‹±æ–‡åŸæ–‡"):
                    st.write(news['summary'])
            
            if news['url']:
                st.markdown(f"ğŸ”— [é˜…è¯»åŸæ–‡]({news['url']})")
        
        with col_side:
            st.markdown(f"**æƒ…ç»ªåˆ†æ:**")
            st.markdown(f"<span style='color:{color}; font-weight:bold; font-size:18px'>{sentiment}</span>", unsafe_allow_html=True)
            
            if news['method'] == 'yfinance':
                st.write("ğŸ¥‡ **é«˜è´¨é‡æº**")
            elif 'Google News' in news['method']:
                st.write("ğŸ¥ˆ **èšåˆæº**")
            else:
                st.write("ğŸ¥‰ **è¡¥å……æº**")
            
            if 'title_zh' in news or 'summary_zh' in news:
                st.write("ğŸŒ **å·²ç¿»è¯‘**")
        
        st.markdown("---")

# ä¸»ç•Œé¢
if st.session_state.news_data is not None:
    news_data = st.session_state.news_data
    source_stats = st.session_state.source_stats
    translated_news = st.session_state.translated_news
    
    if len(news_data) > 0:
        st.subheader("ğŸ“Š å¯é æ•°æ®æºç»Ÿè®¡")
        
        cols = st.columns(len(source_stats) + 2)
        
        total_unique = len(news_data)
        total_raw = sum(source_stats.values())
        
        with cols[0]:
            st.metric("ğŸ“° æœ€ç»ˆç»“æœ", f"{total_unique} æ¡", f"åŸå§‹: {total_raw}")
        
        for i, (source, count) in enumerate(source_stats.items(), 1):
            with cols[i]:
                if count > 0:
                    st.metric(source, f"{count} æ¡", delta="âœ…")
                else:
                    st.metric(source, f"{count} æ¡", delta="âŒ")
        
        with cols[-1]:
            if translated_news:
                translated_count = sum(1 for n in translated_news if 'title_zh' in n or 'summary_zh' in n)
                st.metric("ğŸŒ ç¿»è¯‘çŠ¶æ€", f"{translated_count} æ¡", delta="âœ…")
            else:
                st.metric("ğŸŒ ç¿»è¯‘çŠ¶æ€", "æœªå¯ç”¨", delta="âŒ")
        
        working_sources = len([count for count in source_stats.values() if count > 0])
        total_sources = len(source_stats)
        reliability = working_sources / total_sources * 100
        
        if reliability >= 80:
            st.success(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - ä¼˜ç§€")
        elif reliability >= 60:
            st.warning(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - è‰¯å¥½")
        else:
            st.error(f"ğŸ›¡ï¸ ç³»ç»Ÿå¯é æ€§: {reliability:.0f}% - éœ€è¦æ”¹è¿›")
        
        st.markdown("---")
        
        display_news = translated_news if translated_news else news_data
        
        title_suffix = " (å¼ºåŒ–ç¿»è¯‘ç‰ˆ)" if translated_news else ""
        st.subheader(f"ğŸ“° {ticker or 'å¸‚åœº'} æœ€æ–°æ–°é—»{title_suffix}")
        
        for i, news in enumerate(display_news):
            display_news_item(
                news, 
                i + 1, 
                show_translation=bool(translated_news),
                show_original=show_original if translation_enabled else False
            )
        
        # æƒ…ç»ªç»Ÿè®¡
        st.markdown("### ğŸ“ˆ æ•´ä½“å¸‚åœºæƒ…ç»ªåˆ†æ")
        sentiments = {}
        for news in news_data:
            sentiment, _ = analyze_news_sentiment(news['title'], news['summary'])
            sentiments[sentiment] = sentiments.get(sentiment, 0) + 1
        
        sentiment_cols = st.columns(3)
        for i, (sentiment, count) in enumerate(sentiments.items()):
            with sentiment_cols[i]:
                pct = count / len(news_data) * 100
                if sentiment == 'åˆ©å¥½':
                    st.success(f"ğŸ“ˆ **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
                elif sentiment == 'åˆ©ç©º':
                    st.error(f"ğŸ“‰ **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
                else:
                    st.info(f"ğŸ“Š **{sentiment}**: {count} æ¡ ({pct:.0f}%)")
        
        if translated_news:
            st.markdown("### ğŸŒ ç¿»è¯‘è´¨é‡ç»Ÿè®¡")
            
            # è®¡ç®—APIç¿»è¯‘æˆåŠŸç‡
            api_translated = 0
            local_translated = 0
            
            for news in translated_news:
                if 'title_zh' in news:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯APIç¿»è¯‘ï¼ˆä¸­æ–‡å­—ç¬¦æ¯”ä¾‹é«˜ï¼‰
                    chinese_chars = len([c for c in news['title_zh'] if '\u4e00' <= c <= '\u9fff'])
                    if chinese_chars > len(news['title_zh']) * 0.5:
                        api_translated += 1
                    else:
                        local_translated += 1
            
            trans_cols = st.columns(3)
            with trans_cols[0]:
                st.success(f"ğŸŒ **APIç¿»è¯‘**: {api_translated} æ¡")
            with trans_cols[1]:
                st.info(f"ğŸ“š **æœ¬åœ°ç¿»è¯‘**: {local_translated} æ¡")
            with trans_cols[2]:
                total_translated = api_translated + local_translated
                st.metric("âœ… **ç¿»è¯‘æˆåŠŸç‡**", f"{(total_translated/len(translated_news)*100):.0f}%")
    
    else:
        st.warning("ğŸ“­ æœªè·å–åˆ°æ–°é—»æ•°æ®")

else:
    st.markdown("""
    ## ğŸ¯ æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (å¼ºåŒ–ç¿»è¯‘ç‰ˆ)
    
    ### âš¡ **è§£å†³ç¿»è¯‘é—®é¢˜ï¼Œç¡®ä¿å®Œæ•´ä¸­æ–‡è¾“å‡º**
    
    #### ğŸŒ **å¼ºåŒ–ç¿»è¯‘ç­–ç•¥**
    
    **âŒ ä¹‹å‰çš„é—®é¢˜**:
    ```
    AST SpaceMobile è‚¡ç¥¨ (ASTS) Rallies Alongside Surging Demand for Satellite Internet
    ```
    
    **âœ… ç°åœ¨çš„æ•ˆæœ**:
    ```
    AST SpaceMobileè‚¡ç¥¨(ASTS)å—å«æ˜Ÿäº’è”ç½‘éœ€æ±‚æ¿€å¢æ¨åŠ¨è€Œåå¼¹
    ```
    
    #### ğŸ“Š **ä¸‰é‡ç¿»è¯‘ä¿éšœ**
    - **ğŸ”· ç¬¬1å±‚**: MyMemory API (æœ€ç¨³å®šçš„å…è´¹API)
    - **ğŸ”„ ç¬¬2å±‚**: Googleç¿»è¯‘API (å¤‡ç”¨æ–¹æ¡ˆ)
    - **ğŸ“š ç¬¬3å±‚**: å¼ºåŒ–æœ¬åœ°ç¿»è¯‘ (æ™ºèƒ½å¥å¼è¯†åˆ« + ä¸“ä¸šè¯å…¸)
    
    #### ğŸ¯ **æ ¸å¿ƒæ”¹è¿›**
    - âœ… **ç¡®ä¿å®Œæ•´ç¿»è¯‘**: ä¸å†å‡ºç°åŠè‹±åŠä¸­çš„æƒ…å†µ
    - âœ… **æ™ºèƒ½å¥å¼è¯†åˆ«**: ä¸“é—¨é’ˆå¯¹è´¢ç»æ–°é—»å¥å¼ä¼˜åŒ–
    - âœ… **å®æ—¶è°ƒè¯•**: å¯æŸ¥çœ‹è¯¦ç»†ç¿»è¯‘è¿‡ç¨‹
    - âœ… **è´¨é‡ç»Ÿè®¡**: æ˜¾ç¤ºAPI vs æœ¬åœ°ç¿»è¯‘çš„æ¯”ä¾‹
    
    #### ğŸ§ª **ç«‹å³æµ‹è¯•**
    
    åœ¨å·¦ä¾§"æµ‹è¯•ç¿»è¯‘åŠŸèƒ½"ä¸­å¯ä»¥ï¼š
    - è¾“å…¥ä»»æ„è‹±æ–‡æ–°é—»æ ‡é¢˜
    - å®æ—¶æŸ¥çœ‹ç¿»è¯‘æ•ˆæœ
    - éªŒè¯ç¿»è¯‘è´¨é‡
    
    ### ğŸš€ **ä½¿ç”¨æŒ‡å—**
    
    1. **è¾“å…¥è‚¡ç¥¨ä»£ç **ï¼ˆå¦‚ ASTSï¼‰æˆ–ç•™ç©ºè·å–å¸‚åœºæ–°é—»
    2. **å¯ç”¨å¼ºåŒ–ç¿»è¯‘**
    3. **ç‚¹å‡»è·å–æ–°é—»**
    4. **æŸ¥çœ‹å®Œæ•´ä¸­æ–‡ç¿»è¯‘ç»“æœ**
    
    ### ğŸ’¡ **é¢„æœŸæ•ˆæœ**
    
    - ğŸŒ **APIç¿»è¯‘æˆåŠŸç‡**: 70-90%
    - ğŸ“š **æœ¬åœ°ç¿»è¯‘å…œåº•**: 100%
    - âš¡ **æ•´ä½“ç¿»è¯‘æˆåŠŸç‡**: 100%
    - ğŸ¯ **å®Œæ•´ä¸­æ–‡è¾“å‡º**: ä¿è¯
    
    ---
    
    **ğŸ‘ˆ åœ¨å·¦ä¾§æµ‹è¯•ç¿»è¯‘å¹¶å¼€å§‹ä½¿ç”¨å¼ºåŒ–ç¿»è¯‘åŠŸèƒ½**
    """)

st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
ğŸ“° æœ€æ–°å¯é æ–°é—»ç³»ç»Ÿ (å¼ºåŒ–ç¿»è¯‘ç‰ˆ) | âœ… éªŒè¯æœ‰æ•ˆæº | ğŸ›¡ï¸ 100% ç¿»è¯‘æˆåŠŸç‡ | ğŸŒ å®Œæ•´ä¸­æ–‡è¾“å‡º | ğŸ“± å®¢æˆ·å…å®‰è£…
</div>
""", unsafe_allow_html=True)
